<!DOCTYPE html>
<html lang="ru_RU">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reinforcement learning | My knowledge base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Reinforcement learning" />
<meta property="og:locale" content="ru_RU" />
<meta name="description" content="Обучение с подкреплением" />
<meta property="og:description" content="Обучение с подкреплением" />
<link rel="canonical" href="https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html" />
<meta property="og:url" content="https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html" />
<meta property="og:site_name" content="My knowledge base" />
<script type="application/ld+json">
{"@type":"WebPage","url":"https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html","headline":"Reinforcement learning","description":"Обучение с подкреплением","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="keywords" content="">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    
    
    <link rel="stylesheet" href="https://konstantinklepikov.github.io/myknowlegebase/assets/style.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->


      <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53548570, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53548570" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    <!-- /Yandex.Metrika counter -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139620627-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-139620627-1');
  </script>


<!-- Favicon -->
<link rel="icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- tags collection-->

    







<!-- end custom head snippets -->

</head>

  <body>

    <header class="border-bottom-thick px-2 clearfix">
    <div class="left sm-width-full py-1 mt-1 mt-lg-0">
      <a class="align-middle link-primary text-accent" href="https://konstantinklepikov.github.io/">
        My deep learning
      </a>
    </div>
    <div class="right sm-width-full">
      <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/myknowlegebase/">
            My knowledge base
          </a>
        </li>
      </ul>
    </div>
  </header>

    <main role="main">

      
<article class="container mx-auto px-2 mt2 mb4">
  <header>
    <h1 class="h1 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Reinforcement learning</h1>
  </header>
  <div class="col-4 sm-width-full border-top-thin">
    <p class="mb-3 h5">Теги:
      
        
        <a href="/myknowlegebase/tag/machine-learning" title="machine-learning" class="link-tags">machine-learning&nbsp;</a>
      
        
        <a href="/myknowlegebase/tag/r-learning" title="r-learning" class="link-tags">r-learning&nbsp;</a>
      
      </p>
  </div>
  <div class="prose mb-4 py-4">
    <p>Обучение с подкреплением - это обучение тому, как отображать ситуации на действия, чтобы максимизировать численный сигнал - вознаграждение.</p>

<p>Элементы обучения с подкреплением:</p>

<ul>
  <li><strong>стратегия</strong> определяет то, как обучаемый агент поведет себя в конуретный момент времени. Стратегия отображает множество воспринимаемых состояний среды на действия, предпринимаемые в этих состояниях.</li>
  <li><strong>сигнал вознаграждения</strong> определяет цель обучения. На каждом временном шаге (если задача дискретная) среда посылает агенту число. которое называется вознаграждением. Единственная задача агента - максимизировать полное вознаграждение, полученное в теченеии определенного длительного времени.</li>
  <li><strong>функция ценности</strong> определяет полное вознаграждение, которое агент может ожидать в будущем, если начнет действия в текущем состоянии. Вознограждение определяет текущую желательное состояниес среды, а функция ценности - долговременное желательное состояние, с учетом всех вероятных состояний, которые могут встретиться позже. Вознограждение первично, а функция ценности вторична, однако мы ищем такие состояния, котоыре приносят наибольшую ценность, а не наибольшее вознаграждение в текущий момент, т.к. маленькие вознаграждения могут открыть более ценные состояния в будущем.</li>
  <li><strong>модель</strong> окружающей среды имитирует поведение окружающей среды. Модели используются для <strong>планирования</strong>, подразумевающего любой способ ввода порядка действий путем рассмотрения будущих ситуаций, до того как они фактически произошли. Методы использующие планирование, называются <strong>основанными на моделях</strong>. Не использующие планирование - <strong>безмодельными</strong>. Методы на основе модели опираются на планирование ,а безмодельные методы - на обучение.</li>
</ul>

<p>В простейшем случае (игра крестики-нолики) мы подготавливаем таблицу чисел, по одному для каждого возможного состояния игры. Каждое число - это оценка вероятности выиграть, начав с этого состояния. Это ценность состояния, а вся таблица  - обученная функция ценности. Мы играем много игр с противником. Для выбора хода мы рассматриваем состояния, котоыре получаются в результате каждого возможного хода и оцениваем их ценности по таблице. В основном мы выбираем ход жадно (с наивысшей ценностью), но иногда мы делаем случайный ход (разведочный ход). В процессе игры мы изменяем ценность состояний, в которых оказались. Для этого мы переносим ценность состояний после каждого жадного хода в состояние до этого хода - текущая ценность предыдущего состояния обновляется, так чтобы стать ближе к ценности следующего состояния. Если обозначить <script type="math/tex">S_t</script> состояние до жадного хода, <script type="math/tex">S_{t+1}</script> после, то обновление оценки <script type="math/tex">V(S_t)</script> будет выглядеть так:</p>

<script type="math/tex; mode=display">V(S_t) \leftarrow (S_t) + \alpha[V(S_{t+1}) - V(S_t)]</script>

<p>где <script type="math/tex">\alpha</script> - небольшой положительный коэффициент, называемый параметром размера шага, который влияет на скорость обучения. Это правило обучения - пример обучения на оснвое [<a href="../notes/temporal-difference" title="Temporal difference methods and n-steps methods">temporal-difference</a>].</p>

<p>Применение [<a href="evolution-methods" title="Evolution methods">evolution-methods</a>] к данной задаче означало бы, что мы должны провести прямой поиск в пространстве стратегий, чтобы найти стратегию с высокой вероятностью выигрыша. Для каждой стратегии необходимо было бы получить оценку вероятности выигрыша, сыграв несколько партий с противником, что указывало на то, какую стратегию использовать в следующем шаге. Типичный эволюционный метод - “восхождение на гору” в пространстве стратегий, последовательно генерируя и оценивая стратегии, пытаясь добиться улучшения. Если бы мы использовали генетический алгоритм, то хранили бы и оценивали популяцию стратегий.</p>

<p>Пример демонстрирует различие между эволюционными методами и методами обучения с подкреплением. Для вычисления стратегии эволюционный метод фиксирует стратегию и играет много игр с противником или имитирует много игр с его моделью. Любое измененеие стратегии производится только после статистически значимого числа сыгранных игр, кроме того используется лишь конечный результат - то, что происходит в процессе игры, полностью игнорируется. Методы на основе функции ценности оценивают отдельные состояния, что позволяет задействовать информаци, доступную в процессе самой игры.</p>

<h2 id="методы">Методы</h2>

<p>Методы обучения с подкреплением можно разделить на табличные и приближенные. Табличные методы применимы, когда пространство состояний и действий настолько мало, что его можно описать с помощью таблицы или массива. Приближенные методы обобщают табличные на сколь угодно большое пространство состояний.</p>

<h3 id="табличные-методы">Табличные методы</h3>

<p>К табличным методам можно отнести:</p>

<ul>
  <li>[<a href="../notes/multi-armed-bandit" title="Multy armed bandits">multi-armed-bandit</a>] многорукие бандиты</li>
  <li>[<a href="../notes/mppr" title="MPPR">mppr</a>] конечные марковские процессы принятия решения</li>
  <li>[<a href="../notes/dynamic-programming" title="Dynamic programming for reinforcement-learning">dynamic-programming</a>] динамическое программирование</li>
  <li>[<a href="../notes/monte-carlo" title="Monte-Carlo methods">monte-carlo</a>] методы Монте-Карло</li>
  <li>[<a href="../notes/temporal-difference" title="Temporal difference methods and n-steps methods">temporal-difference</a>] методы, основанные на временных различиях</li>
</ul>

<p>В общем случае методы распределены так:</p>

<p><img src="/myknowlegebase/attachments/2022-10-06-04-41-58.png" alt="methods" /></p>

<p>Кроме того, методы делятся на методы с единой и разделенной стратегией. Кроме того имеют значение следующие аспекты обучения:</p>

<ul>
  <li>определение дохода - эпизодическая задача непрерывная, с обесцениванием или без</li>
  <li>ценность действий, ценность состояний и ценность послесостояний - какие именно ценности следует оценивать</li>
  <li>выбор действия или исследование - требуется соблюсти компромисс между исследованием и использованием</li>
  <li>синхронность/асинхронность</li>
  <li>обучение на реальном опыте, имитации или их соотношении</li>
  <li>обновление пар состояние-действие. В безмодельных возможно обновить только те, которые встречались, в обучении на оснвое модели - произвольные</li>
  <li>обновлять в процессе выбора действий или после выбора</li>
  <li>как долго следует хранить обновленные ценности</li>
</ul>

<h3 id="приближенные-методы">Приближенные методы</h3>

<p>В приближенных методах используется аппроксимирующая функция, применяемая к оцениванию ценности. Можно выделить следующие подходы:</p>

<ul>
  <li>аппроксимация функции ценности при заданной стратегии</li>
  <li>аппроксимация задачи предсказания при заданной стратегии</li>
  <li>аппроксимация функции ценности обучения с раздельной стратегией</li>
  <li>методы следов приемлемости и методы градиента стратегии - в этих методах оптимальная стратегия аппроксимируется непосредственно, а приближенная функция ценности вовсе не используется.</li>
</ul>

<p>К методам градиента стратегий относится алгоритм reinforce и методы исполнитель-критик.</p>

<p>Способы аппроксимации:</p>

<ul>
  <li>стохастические градиентные и полуградиентные методы</li>
  <li>линейные методы (требуется выбор базиса и метода кодирования признаков)</li>
  <li>неелинейные аппроксимации (нейроныне сети)</li>
  <li>метод наименьших квадратов, функции с запоминанием и ядерные функции</li>
</ul>

<h2 id="библиотеки">Библиотеки</h2>

<p><a href="https://winder.ai/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/#google-dopamine-https-github-com-google-dopamine">A Comparison of Reinforcement Learning Frameworks: Dopamine, RLLib, Keras-RL, Coach, TRFL, Tensorforce, Coach and More</a></p>

<ul>
  <li><a href="https://github.com/openai/gym">openai gym</a> A toolkit for developing and comparing reinforcement learning algorithms.</li>
  <li>Google <a href="https://github.com/google/dopamine">dopamine</a>. Dopamine is a research framework for fast prototyping of reinforcement learning algorithms.</li>
  <li><a href="https://docs.ray.io/en/latest/rllib/index.html">RLib</a>. Industry-Grade Reinforcement Learning. <a href="https://github.com/ray-project/ray">github</a></li>
  <li><a href="https://github.com/keras-rl/keras-rl">keras-rl</a>. Deep Reinforcement Learning for Keras.</li>
  <li><a href="https://github.com/deepmind/trfl">TRFL</a>. TensorFlow Reinforcement Learning</li>
  <li><a href="https://github.com/tensorforce/tensorforce">tensorforce</a>. A TensorFlow library for applied reinforcement learning.</li>
  <li>Facebook <a href="https://github.com/facebookresearch/ReAgent">ReAgent</a>. A platform for Reasoning systems (Reinforcement Learning, Contextual Bandits, etc.).</li>
  <li>Intel <a href="https://github.com/IntelLabs/coach">coach</a>. Reinforcement Learning Coach by Intel AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms.</li>
  <li><a href="https://github.com/geek-ai/MAgent">MAgent</a>. A Platform for Many-agent Reinforcement Learning</li>
  <li><a href="https://github.com/tensorflow/agents">TF-Agents</a>. A reliable, scalable and easy to use TensorFlow library for Contextual Bandits and Reinforcement Learning.</li>
  <li><a href="https://github.com/kengz/SLM-Lab">SLM-Lab</a>. Modular Deep Reinforcement Learning framework in [<a href="../notes/pytorch" title="Machine learning framework Pytorch">pytorch</a>]. Companion library of the book “Foundations of Deep Reinforcement Learning”.</li>
  <li><a href="https://github.com/VinF/deer">DeeR</a>. DeeR is a python library for Deep Reinforcement. It is build with modularity in mind so that it can easily be adapted to any need. It provides many possibilities out of the box such as Double Q-learning, prioritized Experience Replay, Deep deterministic policy gradient (DDPG), Combined Reinforcement via Abstract Representations (CRAR). Many different environment examples are also provided (some of them using OpenAI gym).</li>
  <li><a href="https://github.com/rlworkgroup/garage">garage</a>. A toolkit for reproducible reinforcement learning research.</li>
  <li><a href="https://github.com/SurrealAI/surreal">surreal</a>. Open-Source Distributed Reinforcement Learning Framework by Stanford Vision and Learning Lab</li>
  <li><a href="https://github.com/rlgraph/rlgraph">RLgraph</a>. RLgraph is a framework to quickly prototype, define and execute reinforcement learning algorithms both in research and practice. RLgraph is different from most other libraries as it can support TensorFlow (or static graphs in general) or eager/define-by run execution (PyTorch) through a single component interface.</li>
  <li><a href="https://github.com/david-abel/simple_rl">simple_rl</a>. A simple framework for experimenting with Reinforcement Learning in Python.</li>
</ul>

<h2 id="книги">Книги</h2>

<p>Dr. Phil. <a href="https://rl-book.com/">Winder Reinforcement Learning</a>. Industrial Applications with Intelligent Agents</p>

<p>Читай еще:</p>

<ul>
  <li><a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-1-introduction-c0d55c1240a3">The four main subelements of a reinforcement learning system</a></li>
  <li>[<a href="evolution-methods" title="Evolution methods">evolution-methods</a>]</li>
  <li>[<a href="machine-learning" title="Алгоритмы машинного обучения">machine-learning</a>]</li>
</ul>


  </div>
</article>


    </main>

    <footer role="banner">
<div class="border-top-thin clearfix mt-2 mt-lg-4">
    <div class="container mx-auto px-2">
      <p class="col-8 sm-width-full left py-2 mb-0"><a href="/myknowlegebase/">My knowledge base</a> проект поддерживается <a class="text-accent" href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></p>
      <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
        <li class="inline-block mr-1">
          <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="My knowledge base">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
        </li>
      </ul>
    </div>
  </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>