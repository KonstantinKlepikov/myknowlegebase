<!DOCTYPE html>
<html lang="ru_RU">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Reinforcement learning | My knowledge base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Reinforcement learning" />
<meta property="og:locale" content="ru_RU" />
<meta name="description" content="Обучение с подкреплением" />
<meta property="og:description" content="Обучение с подкреплением" />
<link rel="canonical" href="https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html" />
<meta property="og:url" content="https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html" />
<meta property="og:site_name" content="My knowledge base" />
<script type="application/ld+json">
{"@type":"WebPage","url":"https://konstantinklepikov.github.io/myknowlegebase/lists/reinforcement-learning.html","headline":"Reinforcement learning","description":"Обучение с подкреплением","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="keywords" content="">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    
    
    <link rel="stylesheet" href="https://konstantinklepikov.github.io/myknowlegebase/assets/style.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->


      <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53548570, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53548570" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    <!-- /Yandex.Metrika counter -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139620627-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-139620627-1');
  </script>


<!-- Favicon -->
<link rel="icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- tags collection-->

    







<!-- end custom head snippets -->

</head>

  <body>

    <header class="border-bottom-thick px-2 clearfix">
    <div class="left sm-width-full py-1 mt-1 mt-lg-0">
      <a class="align-middle link-primary text-accent" href="https://konstantinklepikov.github.io/">
        My deep learning
      </a>
    </div>
    <div class="right sm-width-full">
      <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/myknowlegebase/">
            My knowledge base
          </a>
        </li>
      </ul>
    </div>
  </header>

    <main role="main">

      
<article class="container mx-auto px-2 mt2 mb4">
  <header>
    <h1 class="h1 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Reinforcement learning</h1>
  </header>
  <div class="col-4 sm-width-full border-top-thin">
    <p class="mb-3 h5">Теги:
      
        
        <a href="/myknowlegebase/tag/machine-learning" title="machine-learning" class="link-tags">machine-learning&nbsp;</a>
      
        
        <a href="/myknowlegebase/tag/r-learning" title="r-learning" class="link-tags">r-learning&nbsp;</a>
      
      </p>
  </div>
  <div class="prose mb-4 py-4">
    <p>Обучение с подкреплением - это обучение тому, как отображать ситуации на действия, чтобы максимизировать численный сигнал - вознаграждение.</p>

<p>Элементы обучения с подкреплением:</p>

<ul>
  <li><strong>стратегия</strong> определяет то, как обучаемый агент поведет себя в конуретный момент времени. Стратегия отображает множество воспринимаемых состояний среды на действия, предпринимаемые в этих состояниях.</li>
  <li><strong>сигнал вознаграждения</strong> определяет цель обучения. На каждом временном шаге (если задача дискретная) среда посылает агенту число. которое называется вознаграждением. Единственная задача агента - максимизировать полное вознаграждение, полученное в теченеии определенного длительного времени.</li>
  <li><strong>функция ценности</strong> определяет полное вознаграждение, которое агент может ожидать в будущем, если начнет действия в текущем состоянии. Вознограждение определяет текущую желательное состояниес среды, а функция ценности - долговременное желательное состояние, с учетом всех вероятных состояний, которые могут встретиться позже. Вознограждение первично, а функция ценности вторична, однако мы ищем такие состояния, котоыре приносят наибольшую ценность, а не наибольшее вознаграждение в текущий момент, т.к. маленькие вознаграждения могут открыть более ценные состояния в будущем.</li>
  <li><strong>модель</strong> окружающей среды имитирует поведение окружающей среды. Модели используются для <strong>планирования</strong>, подразумевающего любой способ ввода порядка действий путем рассмотрения будущих ситуаций, до того как они фактически произошли. Методы использующие планирование, называются <strong>основанными на моделях</strong>. Не использующие планирование - <strong>безмодельными</strong>. Методы на основе модели опираются на планирование ,а безмодельные методы - на обучение.</li>
</ul>

<p>В простейшем случае (игра крестики-нолики) мы подготавливаем таблицу чисел, по одному для каждого возможного состояния игры. Каждое число - это оценка вероятности выиграть, начав с этого состояния. Это ценность состояния, а вся таблица  - обученная функция ценности. Мы играем много игр с противником. Для выбора хода мы рассматриваем состояния, котоыре получаются в результате каждого возможного хода и оцениваем их ценности по таблице. В основном мы выбираем ход жадно (с наивысшей ценностью), но иногда мы делаем случайный ход (разведочный ход). В процессе игры мы изменяем ценность состояний, в которых оказались. Для этого мы переносим ценность состояний после каждого жадного хода в состояние до этого хода - текущая ценность предыдущего состояния обновляется, так чтобы стать ближе к ценности следующего состояния. Если обозначить <script type="math/tex">S_t</script> состояние до жадного хода, <script type="math/tex">S_{t+1}</script> после, то обновление оценки <script type="math/tex">V(S_t)</script> будет выглядеть так:</p>

<script type="math/tex; mode=display">V(S_t) \leftarrow (S_t) + \alpha[V(S_{t+1}) - V(S_t)]</script>

<p>где <script type="math/tex">\alpha</script> - небольшой положительный коэффициент, называемый параметром размера шага, который влияет на скорость обучения. Это правило обучения - пример обучения на оснвое [<a href="../notes/temporal-difference" title="Temporal difference methods and n-steps methods">temporal-difference</a>].</p>

<p>Применение [<a href="evolution-methods" title="Evolution methods">evolution-methods</a>] к данной задаче означало бы, что мы должны провести прямой поиск в пространстве стратегий, чтобы найти стратегию с высокой вероятностью выигрыша. Для каждой стратегии необходимо было бы получить оценку вероятности выигрыша, сыграв несколько партий с противником, что указывало на то, какую стратегию использовать в следующем шаге. Типичный эволюционный метод - “восхождение на гору” в пространстве стратегий, последовательно генерируя и оценивая стратегии, пытаясь добиться улучшения. Если бы мы использовали генетический алгоритм, то хранили бы и оценивали популяцию стратегий.</p>

<p>Пример демонстрирует различие между эволюционными методами и методами обучения с подкреплением. Для вычисления стратегии эволюционный метод фиксирует стратегию и играет много игр с противником или имитирует много игр с его моделью. Любое измененеие стратегии производится только после статистически значимого числа сыгранных игр, кроме того используется лишь конечный результат - то, что происходит в процессе игры, полностью игнорируется. Методы на основе функции ценности оценивают отдельные состояния, что позволяет задействовать информаци, доступную в процессе самой игры.</p>

<h2 id="методы">Методы</h2>

<p>Методы обучения с подкреплением можно разделить на табличные и приближенные. Табличные методы применимы, когда пространство состояний и действий настолько мало, что его можно описать с помощью таблицы или массива. Приближенные методы обобщают табличные на сколь угодно большое пространство состояний.</p>

<p>К табличным методам можно отнести:</p>

<ul>
  <li>[<a href="../notes/multi-armed-bandit" title="Multy armed bandits">multi-armed-bandit</a>] многорукие бандиты</li>
  <li>[<a href="../notes/mppr" title="MPPR">mppr</a>] конечные марковские процессы принятия решения</li>
  <li>[<a href="../notes/dynamic-programming" title="Dynamic programming for reinforcement-learning">notes/dynamic-programming</a>] динамическое программирование</li>
  <li>[<a href="../notes/monte-carlo" title="Monte-Carlo methods">monte-carlo</a>] методы Монте-Карло</li>
  <li>[<a href="../notes/temporal-difference" title="Temporal difference methods and n-steps methods">temporal-difference</a>] методы, основанные на временных различиях</li>
</ul>

<p>В общем случае методы распределены так:</p>

<p><img src="/myknowlegebase/attachments/2022-10-06-04-41-58.png" alt="methods" /></p>

<p>Кроме того, методы делятся на методы с единой и разделенной стратегией. Кроме того имеют значение следующие аспекты обучения:</p>

<ul>
  <li>определение дохода - эпизодическая задача непрерывная, с обесцениванием или без</li>
  <li>ценность действий, ценность состояний и ценность послесостояний - какие именно ценности следует оценивать</li>
  <li>выбор действия или исследование - требуется соблюсти компромисс между исследованием и использованием</li>
  <li>синхронность/асинхронность</li>
  <li>обучение на реальном опыте, имитации или их соотношении</li>
  <li>обновление пар состояние-действие. В безмодельных возможно обновить только те, которые встречались, в обучении на оснвое модели - произвольные</li>
  <li>обновлять в процессе выбора действий или после выбора</li>
  <li>как долго следует хранить обновленные ценности</li>
</ul>

<p>Читай еще:</p>

<ul>
  <li><a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-1-introduction-c0d55c1240a3">The four main subelements of a reinforcement learning system</a></li>
</ul>


  </div>
</article>


    </main>

    <footer role="banner">
<div class="border-top-thin clearfix mt-2 mt-lg-4">
    <div class="container mx-auto px-2">
      <p class="col-8 sm-width-full left py-2 mb-0"><a href="/myknowlegebase/">My knowledge base</a> проект поддерживается <a class="text-accent" href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></p>
      <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
        <li class="inline-block mr-1">
          <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="My knowledge base">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
        </li>
      </ul>
    </div>
  </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>