<!DOCTYPE html>
<html lang="ru_RU">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Crawlers | My knowledge base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Crawlers" />
<meta property="og:locale" content="ru_RU" />
<meta name="description" content="Извлечение данных из web-приложений" />
<meta property="og:description" content="Извлечение данных из web-приложений" />
<link rel="canonical" href="https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html" />
<meta property="og:url" content="https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html" />
<meta property="og:site_name" content="My knowledge base" />
<script type="application/ld+json">
{"description":"Извлечение данных из web-приложений","@type":"WebPage","url":"https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html","headline":"Crawlers","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="keywords" content="">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    
    
    <link rel="stylesheet" href="https://konstantinklepikov.github.io/myknowlegebase/assets/style.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->


      <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53548570, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53548570" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    <!-- /Yandex.Metrika counter -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139620627-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-139620627-1');
  </script>


<!-- Favicon -->
<link rel="icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- tags collection-->

    







<!-- end custom head snippets -->

</head>

  <body>

    <header class="border-bottom-thick px-2 clearfix">
    <div class="left sm-width-full py-1 mt-1 mt-lg-0">
      <a class="align-middle link-primary text-accent" href="https://konstantinklepikov.github.io/">
        My deep learning
      </a>
    </div>
    <div class="right sm-width-full">
      <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/myknowlegebase/">
            My knowledge base
          </a>
        </li>
      </ul>
    </div>
  </header>

    <main role="main">

      
<article class="container mx-auto px-2 mt2 mb4">
  <header>
    <h1 class="h1 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Crawlers</h1>
  </header>
  <div class="col-4 sm-width-full border-top-thin">
    <p class="mb-3 h5">Теги:
      
        
        <a href="/myknowlegebase/tag/crawlers" title="crawlers" class="link-tags">crawlers&nbsp;</a>
      
    </p>
  </div>
  <div>
    <script type="text/javascript"> function googleTranslateElementInit() { new google.translate.TranslateElement({pageLanguage: 'ru'}, 'google_translate_element'); } </script> <script type="text/javascript" src="//[translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
  </div>
  <div class="prose mb-4 py-4">
    <h2 id="что-такое-веб-краулеры">Что такое веб-краулеры</h2>

<p>Веб-краулер - интернет интернет-бот, который систематически просматривает web и обычно используется поисковыми системами для веб-индексации.</p>

<p>Сканеры могут проверять гиперссылки и HTML - код. Их также можно использовать для парсинга веб - страниц и программирования на основе данных.</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-21-34-02.png" alt="crowler" /></p>

<p>Поисковый робот начинает со списка URL-адресов для посещения. Эти первые URL-адреса называются “посевными”. Когда сканер посещает эти URL-адреса, он идентифицирует все гиперссылки на извлеченных веб-страницах и добавляет их в список URL-адресов для посещения, называемый “границей сканирования”. URL-адреса рекурсивно посещаются в соответствии с набором политик. Если сканер выполняет архивирование веб-сайтов, он копирует и сохраняет информацию по мере ее поступления. Архивы обычно хранятся таким образом, чтобы их можно было просматривать, читать и перемещаться по ним, как если бы они находились в реальном времени в интернете. Страницы сохраняются в виде снапшотов.</p>

<p>Архив известен как репозиторий и предназначен для хранения и управления коллекцией веб-страниц. В репозитории хранятся только HTML-страницы обычно в виде отдельных файлов. Репозиторий похож на любую другую систему хранения данных, например, на современную базу данных. Единственное отличие состоит в том, что репозиторию не нужны все функции, предлагаемые системой баз данных.</p>

<p>Большой объем подразумевает, что сканер может сохранять только ограниченное количество веб-страниц за заданное время, поэтому ему необходимо расставить приоритеты при загрузке.</p>

<p>Количество возможных просканированных URL-адресов, генерируемых серверным программным обеспечением, также затрудняет поиск дублирующегося контента для поисковых роботов. Существуют бесконечные комбинации параметров на основе URL-адреса, из которых только небольшой выбор действительно возвращает уникальный контент. Например, простая онлайн-галерея фотографий может существовавть четыре способа сортировки изображений, три варианта размера эскизов, два формата файлов и возможность отключить пользовательский контент, поэтому к одному и тому же набору контента можно получить доступ с 48 различных URL-адресов. Эта математическая комбинация создает проблему для поисковых роботов, поскольку они должны сортировать бесконечные комбинации относительно незначительных изменений, чтобы получить уникальный контент. Поисковый робот должен тщательно выбирать на каждом шагу, какие страницы посещать дальше.</p>

<h2 id="политики">Политики</h2>

<p>Поведение поискового робота является результатом комбинации политик:</p>

<ul>
  <li>политика выбора, в которой указаны страницы для загрузки</li>
  <li>политика повторного посещения, в которой указано, когда следует проверять наличие изменений на страницах</li>
  <li>политика вежливости, указывающая, как избежать перегрузки веб-сайтов</li>
  <li>политика параллелизма, в которой указано, как координировать распределенные поисковые роботы.</li>
</ul>

<h3 id="политика-отбора">Политика отбора</h3>

<p>Необходима метрика важоности страницы, зависящая от внутреннего качества, популярности с точки зрения ссылок и посещения, а так-же url-адреса. Политика должна работать с частисной информацией, так-как полный набор веб-страниц во время обхода неизщвестен.</p>

<p>К методам формирования политики относится:</p>

<ul>
  <li>ограничение перехода по ссылкам (игнорирование определенных тоипов MIME или адресов с определенным содержанием)</li>
  <li>нормализация url</li>
  <li>сканирвоание по восходящему пути</li>
  <li><a href="https://en.wikipedia.org/wiki/Focused_crawler">целенаправленное сканирование</a></li>
</ul>

<h3 id="политика-повторного-посещения">Политика повторного посещения</h3>

<p>Определяет посещение страниц с учетом вероятности их измененеия. Рассматривают два критерия, определяющих политики провторного посещения:</p>

<ul>
  <li>Freshness - это мера, которая указывает, является ли локальная копия точной или нет</li>
  <li>Age - это мера, показывающая, насколько устарела локальная копия</li>
</ul>

<p>Свежесть страницы p в репозитории в момент времени t определяется как:</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-20-28.png" alt="Freshness" /></p>

<p>Возраст страницы p в репозитории в момент времени t определяется как:</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-21-41.png" alt="Age" /></p>

<p>Аналогом “свежести” явлется другая цель - минимизация доли времени, в течение которого страницы остаются устаревшими. Проблему веб-сканирования можно смоделировать как систему опроса с несколькими очередями и одним сервером, в которой веб-сканер является сервером, а веб-сайты — очередями. <a href="https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1425(199806)1:1%3C15::AID-JOS3%3E3.0.CO;2-K">Подробнее</a></p>

<p>Задача краулера — поддерживать как можно более высокую среднюю свежесть страниц в своей коллекции или поддерживать как можно более низкий средний возраст страниц. Эти цели не равнозначны: в первом случае краулера просто интересует, сколько страниц устарело, а во втором случае краулер интересует, какой возраст у локальных копий страниц.</p>

<p>Наиболее простые стратегии выглядят так:</p>

<ul>
  <li>Единая политика предполагает повторное посещение всех страниц в коллекции с одинаковой частотой, независимо от скорости их изменения</li>
  <li>Gропорциональная политика - предполагает более частое повторное посещение страниц, которые меняются чаще. Частота посещений прямо пропорциональна (оценочной) частоте изменений</li>
</ul>

<p>В обоих случаях повторный порядок сканирования страниц может выполняться либо в случайном, либо в фиксированном порядке. C точки зрения средней свежести единая политика превосходит пропорциональную политику как в симуляции, так и в реальном веб-сканировании. Интуитивно причина заключается в том, что, поскольку у поисковых роботов есть ограничение на количество страниц, которые они могут просканировать за заданный период времени, они будут выделять слишком много новых обходов для быстро меняющихся страниц за счет менее часто обновляемых страниц, и свежесть быстро меняющихся страниц сохраняется в течение более короткого периода, чем свежесть менее часто меняющихся страниц.</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-49-45.png" alt="politics" /></p>

<p>Требуется взвешенный подход - краулер должен “наказывать” слишком быстро обновляемые страницы. Оптимальный метод поддержания высокой средней свежести включает игнорирование страниц, которые меняются слишком часто, а оптимальный способ поддержания низкого среднего возраста — использование частот доступа, которые монотонно (и сублинейно) увеличиваются со скоростью изменения каждой страницы. В обоих случаях оптимум ближе к единообразной политике, чем к пропорциональной.</p>

<h3 id="политика-вежливости">Политика вежливости</h3>

<p>Работа крайлера оказывает негативное влияние на производительность ресурса из-за потенциальной загрузки цП и диска. Использование протокла <code class="language-plaintext highlighter-rouge">robots.txt</code> устанавливает лимиты для “добропорядочных” пауков.</p>

<p>Неофициальные данные из журналов доступа показывают, что интервалы доступа от известных поисковых роботов варьируются от 20 секунд до 3–4 минут. Стандартом считается выбор интервала в 10-15с после ответа на последний запрос.</p>

<h3 id="политика-параллелизма">Политика параллелизма</h3>

<p><a href="https://en.wikipedia.org/wiki/Distributed_web_crawling">Распараллеливание процессов</a> - обычная практика при обходе страниц в сети, так как общее время обхода в рамках одного процесса может создавать неприемлемый горизонт.</p>

<h2 id="идентификация-сканера">Идентификация сканера</h2>

<p>Веб-сканеры обычно идентифицируют себя для веб-сервера, используя поле <code class="language-plaintext highlighter-rouge">User-agent</code> в HTTP - запросе. Администраторы веб-сайтов обычно просматривают журнал своих веб-серверов и используют поле пользовательского агента, чтобы определить, какие поисковые роботы посещали веб-сервер и как часто. Поле пользовательского агента может включать URL-адрес, по которому администратор веб-сайта может найти дополнительную информацию об искателе. Изучение журнала веб-сервера — утомительная задача, поэтому некоторые администраторы используют инструменты для идентификации, отслеживания и проверки поисковых роботов. Спам-боты и другие вредоносные поисковые роботы вряд ли помещают идентифицирующую информацию в поле пользовательского агента или могут маскировать свою личность под браузер или другой известный поисковый робот.</p>

<p>Администраторы веб-сайтов предпочитают, чтобы поисковые роботы идентифицировали себя, чтобы при необходимости они могли связаться с владельцем. В некоторых случаях сканеры могут случайно попасть в ловушку для сканеров или могут перегружать веб-сервер запросами, и владельцу необходимо остановить сканер. Идентификация также полезна для администраторов, которым интересно знать, когда их веб-страницы могут быть проиндексированы той или иной поисковой системой .</p>

<h3 id="sitemapsxml">Sitemaps.xml</h3>

<p>Используется для предоставлению доступа к страницам, к которым сложно добраться в результате обхода.</p>

<p><a href="https://en.wikipedia.org/wiki/Web_crawler">Оригинал статьи в вики</a></p>

<h2 id="все-заметки-в-этой-тематике">Все заметки в этой тематике</h2>

<ul>
  <li>[<a href="../notes/scrapy" title="Scrapy">scrapy</a>]</li>
  <li>[<a href="../notes/playwright" title="Playwright">playwright</a>]</li>
  <li>[<a href="../notes/splash" title="Splash">splash</a>]</li>
  <li>[<a href="../notes/selenium" title="Selenium">selenium</a>]</li>
  <li>[<a href="../notes/xpath" title="XPath в scrapy">xpath</a>]</li>
  <li>[<a href="../notes/css-selectors" title="Css-selectors">css-selectors</a>]</li>
  <li>[<a href="../notes/xpath-css-examples" title="XPath css examples">xpath-css-examples</a>]</li>
  <li>[<a href="../notes/beautifulsoup" title="BeautifulSoup">BeautifulSoup</a>]</li>
  <li>[<a href="../notes/urllibparse" title="Urllib.parse - парсинг урлов в компоненты">urllibparse</a>]</li>
  <li>[<a href="../notes/scrapyd" title="Scrapyd">scrapyd</a>]</li>
</ul>

<p>Дополнительно:</p>

<ul>
  <li>[<a href="../posts/2022-01-04-daily-note" title="Proxy в selenium, запуск локального smtp и несколько вопросов про pandas">2022-01-04-daily-note</a>] использование proxy в selenium</li>
  <li>[<a href="../posts/2022-02-04-daily-note" title="Работа в selenium с firefox">2022-02-04-daily-note</a>] про вебдрайверы в [<a href="../notes/selenium" title="Selenium">selenium</a>]</li>
  <li><a href="https://www.zenrows.com/blog/web-scraping-with-selenium-in-python#getting-started">Web Scraping with Selenium in Python</a></li>
  <li><a href="https://www.zenrows.com/blog/stealth-web-scraping-in-python-avoid-blocking-like-a-ninja#behavioral-patterns">про обход блокировок при скрапинге</a></li>
</ul>

<p>Пакеты и либы:</p>

<ul>
  <li>
    <p><a href="https://github.com/mediacloud/ultimate-sitemap-parser">ultimate-sitemap-parser</a></p>
  </li>
  <li><a href="https://scrapyd.readthedocs.io/en/stable/index.html">Scrapyd - an application for deploying and running Scrapy spiders</a></li>
  <li><a href="https://docs.gerapy.com/en/latest/">Gerapy</a> - деплоер для [<a href="../notes/scrapy" title="Scrapy">scrapy</a>], реализует деплой пауков, таскменеджмент и админку</li>
  <li><a href="https://github.com/clemfromspace/scrapy-selenium">scrapy-silenium</a> [<a href="../notes/selenium" title="Selenium">selenium</a>] в качестве мидлвейра для [<a href="../notes/scrapy" title="Scrapy">scrapy</a>]</li>
  <li><a href="https://github.com/scrapy-plugins/scrapy-playwright">scrapy-playwright</a> Playwright integration for Scrapy</li>
  <li><a href="https://github.com/TeamHG-Memex/scrapy-rotating-proxies">scrapy-rotating-proxies</a> This package provides a Scrapy middleware to use rotating proxies, check that they are alive and adjust crawling speed.</li>
  <li><a href="https://scrapy-cluster.readthedocs.io/en/latest/index.html">Scrapy Cluster</a> Scrapy based distributed crawling project, Scrapy Cluster.</li>
  <li>
    <p>[<a href="../notes/scrapyd" title="Scrapyd">scrapyd</a>] is an application for deploying and running Scrapy spiders. It enables you to deploy (upload) your projects and control their spiders using a JSON API.</p>
  </li>
  <li><a href="https://github.com/wkeeling/selenium-wire">selenium-wire</a> extends Selenium’s Python bindings to give you access to the underlying requests made by the browser. You author your code in the same way as you do with Selenium, but you get extra APIs for inspecting requests and responses and making changes to them on the fly</li>
  <li>
    <p><a href="https://github.com/SeleniumHQ/docker-selenium">docker-selenium</a> Docker images for the Selenium Grid Server</p>
  </li>
  <li><a href="https://github.com/alertot/detectem">detectem</a> specialized software web detector (command line tool)</li>
  <li>
    <p><a href="https://github.com/richardpenman/builtwith">builtwith</a> info about web-source engines <a href="https://github.com/varnitsingh/builtwith/commit/aa23671d968f5ad0e64a059a30b5ebb9bdbaa56f">look at solution of problem with long waiting</a></p>
  </li>
  <li>
    <p><a href="https://playwright.dev/python/docs/intro">playwright</a> enables reliable end-to-end testing for modern web apps</p>
  </li>
  <li><a href="https://github.com/spyoungtech/grequests">grequests</a> GRequests allows you to use Requests with Gevent to make asynchronous HTTP Requests easily.</li>
  <li><a href="https://github.com/ross/requests-futures">requests-futures</a> Asynchronous Python HTTP Requests for Human</li>
  <li>
    <p><a href="https://github.com/requests/requests-threads">requests-threads</a> This repo contains a Requests session that returns the amazing Twisted’s awaitable Deferreds instead of Response objects</p>
  </li>
  <li><a href="https://github.com/alirezamika/autoscraper">autoscraper</a> A Smart, Automatic, Fast and Lightweight Web Scraper for Python</li>
  <li><a href="https://github.com/roniemartinez/dude">dude</a> Dude is a very simple framework for writing a web scraper using Python decorators. The design, inspired by Flask, was to easily build a web scraper in just a few lines of code. Dude has an easy-to-learn syntax.</li>
  <li><a href="https://splash.readthedocs.io/en/stable/">Splash - A javascript rendering service</a></li>
  <li><a href="https://github.com/scrapinghub/portia">portia</a> crawler with ui interface</li>
  <li><a href="https://github.com/psf/requests-html">requests-html</a> This library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible</li>
  <li>
    <p><a href="https://github.com/goldsmith/Wikipedia">Wikipedia</a> is a Python library that makes it easy to access and parse data from Wikipedia</p>
  </li>
  <li>
    <p><a href="https://robotframework.org/">Robot Framework</a> is a generic open source automation framework. It can be used for test automation and robotic process automation (RPA).</p>
  </li>
  <li>[<a href="../notes/feedparser" title="Feedparser - rss и atom парсинг">feedparser</a>]</li>
</ul>

<p>Небольшая проблема с requests-html</p>

<script src="https://gist.github.com/1867a45b9e7a7490e055d4932ac912d3.js"> </script>


  </div>
</article>


    </main>

    <footer role="banner">
<div class="border-top-thin clearfix mt-2 mt-lg-4">
    <div class="container mx-auto px-2">
      <p class="col-8 sm-width-full left py-2 mb-0"><a href="/myknowlegebase/">My knowledge base</a> проект поддерживается <a class="text-accent" href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></p>
      <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
        <li class="inline-block mr-1">
          <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="My knowledge base">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
        </li>
      </ul>
    </div>
  </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>