<!DOCTYPE html>
<html lang="ru-RU">

<html>

  <head>

    <meta charset="UTF-8">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Crawlers | My knowlege base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Crawlers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Извлечение данных из web-приложений" />
<meta property="og:description" content="Извлечение данных из web-приложений" />
<link rel="canonical" href="https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html" />
<meta property="og:url" content="https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html" />
<meta property="og:site_name" content="My knowlege base" />
<script type="application/ld+json">
{"@type":"WebPage","url":"https://konstantinklepikov.github.io/myknowlegebase/lists/crawlers.html","headline":"Crawlers","description":"Извлечение данных из web-приложений","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style"
        type="text/css" crossorigin>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    
    
    <link rel="stylesheet" href="https://konstantinklepikov.github.io/myknowlegebase/assets/css/style.css">

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->


  <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53548570, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53548570" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    <!-- /Yandex.Metrika counter -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139620627-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-139620627-1');
  </script>


<!-- Favicon -->
<link rel="icon" href="/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

</head>

  <body>

    <header role="banner">
    <div class="container">
        <h1 id="a-title"><a href="/myknowlegebase/">My knowlege base</a></h1>
        <h2 class="project-tagline">Мои заметки о программировании, data science и алгоритмах, собранные в процессе обучения</h2>
        <p>Тут собраны заметки по программированию, машинному обучению и алгоритмам, которые автор <a href=""></a> делал и продолжает делать в процессе обучения. Тысяча извинений за сумбурность записей, орфографию и изрядную долю копипасты. По сути это конспект. Более толковые статьи можно почитать в блоге <a href="https://konstantinklepikov.github.io/">my deep learning</a></p>
    </div>
</header>

    <main id="main-content" class="container" role="main">

      <h1 id="crawlers">Crawlers</h1>

<h2 id="что-такое-веб-краулеры">Что такое веб-краулеры</h2>

<p>Веб-краулер - интернет интернет-бот, который систематически просматривает web и обычно используется поисковыми системами для веб-индексации.</p>

<p>Сканеры могут проверять гиперссылки и HTML - код. Их также можно использовать для парсинга веб - страниц и программирования на основе данных.</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-21-34-02.png" alt="crowler" /></p>

<p>Поисковый робот начинает со списка URL-адресов для посещения. Эти первые URL-адреса называются “посевными”. Когда сканер посещает эти URL-адреса, он идентифицирует все гиперссылки на извлеченных веб-страницах и добавляет их в список URL-адресов для посещения, называемый “границей сканирования”. URL-адреса рекурсивно посещаются в соответствии с набором политик. Если сканер выполняет архивирование веб-сайтов, он копирует и сохраняет информацию по мере ее поступления. Архивы обычно хранятся таким образом, чтобы их можно было просматривать, читать и перемещаться по ним, как если бы они находились в реальном времени в интернете. Страницы сохраняются в виде снапшотов.</p>

<p>Архив известен как репозиторий и предназначен для хранения и управления коллекцией веб-страниц. В репозитории хранятся только HTML-страницы обычно в виде отдельных файлов. Репозиторий похож на любую другую систему хранения данных, например, на современную базу данных. Единственное отличие состоит в том, что репозиторию не нужны все функции, предлагаемые системой баз данных.</p>

<p>Большой объем подразумевает, что сканер может сохранять только ограниченное количество веб-страниц за заданное время, поэтому ему необходимо расставить приоритеты при загрузке.</p>

<p>Количество возможных просканированных URL-адресов, генерируемых серверным программным обеспечением, также затрудняет поиск дублирующегося контента для поисковых роботов. Существуют бесконечные комбинации параметров на основе URL-адреса, из которых только небольшой выбор действительно возвращает уникальный контент. Например, простая онлайн-галерея фотографий может существовавть четыре способа сортировки изображений, три варианта размера эскизов, два формата файлов и возможность отключить пользовательский контент, поэтому к одному и тому же набору контента можно получить доступ с 48 различных URL-адресов. Эта математическая комбинация создает проблему для поисковых роботов, поскольку они должны сортировать бесконечные комбинации относительно незначительных изменений, чтобы получить уникальный контент. Поисковый робот должен тщательно выбирать на каждом шагу, какие страницы посещать дальше.</p>

<h2 id="политики">Политики</h2>

<p>Поведение поискового робота является результатом комбинации политик:</p>

<ul>
  <li>политика выбора, в которой указаны страницы для загрузки</li>
  <li>политика повторного посещения, в которой указано, когда следует проверять наличие изменений на страницах</li>
  <li>политика вежливости, указывающая, как избежать перегрузки веб-сайтов</li>
  <li>политика параллелизма, в которой указано, как координировать распределенные поисковые роботы.</li>
</ul>

<h3 id="политика-отбора">Политика отбора</h3>

<p>Необходима метрика важоности страницы, зависящая от внутреннего качества, популярности с точки зрения ссылок и посещения, а так-же url-адреса. Политика должна работать с частисной информацией, так-как полный набор веб-страниц во время обхода неизщвестен.</p>

<p>К методам формирования политики относится:</p>

<ul>
  <li>ограничение перехода по ссылкам (игнорирование определенных тоипов MIME или адресов с определенным содержанием)</li>
  <li>нормализация url</li>
  <li>сканирвоание по восходящему пути</li>
  <li><a href="https://en.wikipedia.org/wiki/Focused_crawler">целенаправленное сканирование</a></li>
</ul>

<h3 id="политика-повторного-посещения">Политика повторного посещения</h3>

<p>Определяет посещение страниц с учетом вероятности их измененеия. Рассматривают два критерия, определяющих политики провторного посещения:</p>

<ul>
  <li>Freshness - это мера, которая указывает, является ли локальная копия точной или нет</li>
  <li>Age - это мера, показывающая, насколько устарела локальная копия</li>
</ul>

<p>Свежесть страницы p в репозитории в момент времени t определяется как:</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-20-28.png" alt="Freshness" /></p>

<p>Возраст страницы p в репозитории в момент времени t определяется как:</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-21-41.png" alt="Age" /></p>

<p>Аналогом “свежести” явлется другая цель - минимизация доли времени, в течение которого страницы остаются устаревшими. Проблему веб-сканирования можно смоделировать как систему опроса с несколькими очередями и одним сервером, в которой веб-сканер является сервером, а веб-сайты — очередями. <a href="https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-1425(199806)1:1%3C15::AID-JOS3%3E3.0.CO;2-K">Подробнее</a></p>

<p>Задача краулера — поддерживать как можно более высокую среднюю свежесть страниц в своей коллекции или поддерживать как можно более низкий средний возраст страниц. Эти цели не равнозначны: в первом случае краулера просто интересует, сколько страниц устарело, а во втором случае краулер интересует, какой возраст у локальных копий страниц.</p>

<p>Наиболее простые стратегии выглядят так:</p>

<ul>
  <li>Единая политика предполагает повторное посещение всех страниц в коллекции с одинаковой частотой, независимо от скорости их изменения</li>
  <li>Gропорциональная политика - предполагает более частое повторное посещение страниц, которые меняются чаще. Частота посещений прямо пропорциональна (оценочной) частоте изменений</li>
</ul>

<p>В обоих случаях повторный порядок сканирования страниц может выполняться либо в случайном, либо в фиксированном порядке. C точки зрения средней свежести единая политика превосходит пропорциональную политику как в симуляции, так и в реальном веб-сканировании. Интуитивно причина заключается в том, что, поскольку у поисковых роботов есть ограничение на количество страниц, которые они могут просканировать за заданный период времени, они будут выделять слишком много новых обходов для быстро меняющихся страниц за счет менее часто обновляемых страниц, и свежесть быстро меняющихся страниц сохраняется в течение более короткого периода, чем свежесть менее часто меняющихся страниц.</p>

<p><img src="/myknowlegebase/attachments/2022-01-14-22-49-45.png" alt="politics" /></p>

<p>Требуется взвешенный подход - краулер должен “наказывать” слишком быстро обновляемые страницы. Оптимальный метод поддержания высокой средней свежести включает игнорирование страниц, которые меняются слишком часто, а оптимальный способ поддержания низкого среднего возраста — использование частот доступа, которые монотонно (и сублинейно) увеличиваются со скоростью изменения каждой страницы. В обоих случаях оптимум ближе к единообразной политике, чем к пропорциональной.</p>

<h3 id="политика-вежливости">Политика вежливости</h3>

<p>Работа крайлера оказывает негативное влияние на производительность ресурса из-за потенциальной загрузки цП и диска. Использование протокла <code class="language-plaintext highlighter-rouge">robots.txt</code> устанавливает лимиты для “добропорядочных” пауков.</p>

<p>Неофициальные данные из журналов доступа показывают, что интервалы доступа от известных поисковых роботов варьируются от 20 секунд до 3–4 минут. Стандартом считается выбор интервала в 10-15с после ответа на последний запрос.</p>

<h3 id="политика-параллелизма">Политика параллелизма</h3>

<p><a href="https://en.wikipedia.org/wiki/Distributed_web_crawling">Распараллеливание процессов</a> - обычная практика при обходе страниц в сети, так как общее время обхода в рамках одного процесса может создавать неприемлемый горизонт.</p>

<h2 id="идентификация-сканера">Идентификация сканера</h2>

<p>Веб-сканеры обычно идентифицируют себя для веб-сервера, используя поле <code class="language-plaintext highlighter-rouge">User-agent</code> в HTTP - запросе. Администраторы веб-сайтов обычно просматривают журнал своих веб-серверов и используют поле пользовательского агента, чтобы определить, какие поисковые роботы посещали веб-сервер и как часто. Поле пользовательского агента может включать URL-адрес, по которому администратор веб-сайта может найти дополнительную информацию об искателе. Изучение журнала веб-сервера — утомительная задача, поэтому некоторые администраторы используют инструменты для идентификации, отслеживания и проверки поисковых роботов. Спам-боты и другие вредоносные поисковые роботы вряд ли помещают идентифицирующую информацию в поле пользовательского агента или могут маскировать свою личность под браузер или другой известный поисковый робот.</p>

<p>Администраторы веб-сайтов предпочитают, чтобы поисковые роботы идентифицировали себя, чтобы при необходимости они могли связаться с владельцем. В некоторых случаях сканеры могут случайно попасть в ловушку для сканеров или могут перегружать веб-сервер запросами, и владельцу необходимо остановить сканер. Идентификация также полезна для администраторов, которым интересно знать, когда их веб-страницы могут быть проиндексированы той или иной поисковой системой .</p>

<h3 id="sitemapsxml">Sitemaps.xml</h3>

<p>Используется для предоставлению доступа к страницам, к которым сложно добраться в результате обхода.</p>

<p><a href="https://en.wikipedia.org/wiki/Web_crawler">Оригинал статьи в вики</a></p>

<h2 id="все-заметки-в-этой-тематике">Все заметки в этой тематике</h2>

<ul>
  <li>[<a href="../notes/scrapy" title="Scrapy">scrapy</a>]</li>
  <li>[<a href="../notes/selenium" title="Selenium">selenium</a>]</li>
  <li>[<a href="../notes/xpath" title="XPath">xpath</a>]</li>
  <li>[<a href="../notes/css-selectors" title="Css-selectors">css-selectors</a>]</li>
  <li>[<a href="../notes/beautifulsoup" title="BeautifulSoup">BeautifulSoup</a>]</li>
  <li>[<a href="../posts/2022-01-04-daily-note" title="Proxy в selenium, запуск локального smtp и несколько вопросов про pandas">2022-01-04-daily-note</a>] использование proxy в selenium</li>
</ul>

<p>Либы:</p>

<ul>
  <li><a href="https://github.com/mediacloud/ultimate-sitemap-parser">ultimate-sitemap-parser</a></li>
  <li><a href="https://scrapyd.readthedocs.io/en/stable/index.html">Scrapyd - an application for deploying and running Scrapy spiders</a></li>
  <li><a href="https://github.com/clemfromspace/scrapy-selenium">scrapy-silenium</a> [<a href="../notes/selenium" title="Selenium">selenium</a>] в качестве мидлвейра для [<a href="../notes/scrapy" title="Scrapy">scrapy</a>]</li>
  <li><a href="https://github.com/alertot/detectem">detectem</a> specialized software web detector (command line tool)</li>
  <li><a href="https://github.com/richardpenman/builtwith">builtwith</a> info about web-source engines <a href="https://github.com/varnitsingh/builtwith/commit/aa23671d968f5ad0e64a059a30b5ebb9bdbaa56f">look at solution of problem with long waiting</a></li>
  <li><a href="https://github.com/wkeeling/selenium-wire">selenium-wire</a> extends Selenium’s Python bindings to give you access to the underlying requests made by the browser. You author your code in the same way as you do with Selenium, but you get extra APIs for inspecting requests and responses and making changes to them on the fly</li>
  <li><a href="https://github.com/psf/requests-html">requests-html</a> This library intends to make parsing HTML (e.g. scraping the web) as simple and intuitive as possible</li>
</ul>

<script src="https://gist.github.com/1867a45b9e7a7490e055d4932ac912d3.js"> </script>



<div class="additional-pad">
  <p><a href="/myknowlegebase/">>>> На главную</a></p>
</div>


    </main>

    <footer role="banner">
    <div class="container">
        <h4><a href="/myknowlegebase/">My knowlege base</a> поддерживается <a href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></h4>
        <h4>Блог автора: <a href="https://konstantinklepikov.github.io/">My deep learning</a></h4>
    </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>