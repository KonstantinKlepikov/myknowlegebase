<!DOCTYPE html>
<html lang="ru_RU">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Scrapy | My knowledge base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Scrapy" />
<meta property="og:locale" content="ru_RU" />
<meta name="description" content="Высокоуровневый фреймворк для web-скрапинга и краулинга на python" />
<meta property="og:description" content="Высокоуровневый фреймворк для web-скрапинга и краулинга на python" />
<link rel="canonical" href="https://konstantinklepikov.github.io/myknowlegebase/notes/scrapy.html" />
<meta property="og:url" content="https://konstantinklepikov.github.io/myknowlegebase/notes/scrapy.html" />
<meta property="og:site_name" content="My knowledge base" />
<script type="application/ld+json">
{"description":"Высокоуровневый фреймворк для web-скрапинга и краулинга на python","@type":"WebPage","url":"https://konstantinklepikov.github.io/myknowlegebase/notes/scrapy.html","headline":"Scrapy","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="keywords" content="">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    
    
    <link rel="stylesheet" href="https://konstantinklepikov.github.io/myknowlegebase/assets/style.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->


      <!-- Yandex.Metrika counter -->
  <script type="text/javascript" >
    (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
    m[i].l=1*new Date();k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
    (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

    ym(53548570, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
    });
  </script>
  <noscript><div><img src="https://mc.yandex.ru/watch/53548570" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
    <!-- /Yandex.Metrika counter -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-139620627-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-139620627-1');
  </script>


<!-- Favicon -->
<link rel="icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="https://konstantinklepikov.github.io/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- tags collection-->

    







<!-- end custom head snippets -->

</head>

  <body>

    <header class="border-bottom-thick px-2 clearfix">
    <div class="left sm-width-full py-1 mt-1 mt-lg-0">
      <a class="align-middle link-primary text-accent" href="https://konstantinklepikov.github.io/">
        My deep learning
      </a>
    </div>
    <div class="right sm-width-full">
      <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/myknowlegebase/">
            My knowledge base
          </a>
        </li>
      </ul>
    </div>
  </header>

    <main role="main">

      
<article class="container mx-auto px-2 mt2 mb4">
  <header>
    <h1 class="h1 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Scrapy</h1>
  </header>
  <div class="col-4 sm-width-full border-top-thin">
    <p class="mb-3 h5">Теги:
      
        
        <a href="/myknowlegebase/tag/crawlers" title="crawlers" class="link-tags">crawlers&nbsp;</a>
      
        
        <a href="/myknowlegebase/tag/python" title="python" class="link-tags">python&nbsp;</a>
      
    </p>
  </div>
  <div>
    <script type="text/javascript"> function googleTranslateElementInit() { new google.translate.TranslateElement({pageLanguage: 'ru'}, 'google_translate_element'); } </script> <script type="text/javascript" src="//[translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
  </div>
  <div class="prose mb-4 py-4">
    <p>Scrapy - это dысокоуровневый fcby[hjyysq фреймворк для web-скрапинга и краулинга на python</p>

<p>Создание проекта: <code class="language-plaintext highlighter-rouge">scrapy startproject proj_name</code></p>

<p>Результат:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>proj_name/
    scrapy.cfg            <span class="c"># deploy configuration file</span>

    proj_name/            <span class="c"># project's Python module, you'll import your code from here</span>
        __init__.py

        items.py          <span class="c"># project items definition file</span>

        middlewares.py    <span class="c"># project middlewares file</span>

        pipelines.py      <span class="c"># project pipelines file</span>

        settings.py       <span class="c"># project settings file</span>

        spiders/          <span class="c"># a directory where you'll later put your spiders</span>
            __init__.py
</code></pre></div></div>

<p>Краулер можно создать, наслудуясь от класса <code class="language-plaintext highlighter-rouge">Spider</code> в spiders/</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"quotes"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://quotes.toscrape.com/page/1/'</span><span class="p">,</span>
        <span class="s">'http://quotes.toscrape.com/page/2/'</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">page</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">"/"</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s">f'quotes-</span><span class="si">{</span><span class="n">page</span><span class="si">}</span><span class="s">.html'</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">body</span><span class="p">)</span>
</code></pre></div></div>

<p>АПИ scrapy активно использует колбеки (смотри [<a href="python-glossary" title="Python glossary">python-glossary</a>]). <code class="language-plaintext highlighter-rouge">parse()</code> метод будет вызываться для обработки каждого запроса URL-адресов, даже если мы явно не указали Scrapy делать это. Это происходит потому, что <code class="language-plaintext highlighter-rouge">parse()</code> это колбек по умолчанию, который вызывается для запросов без явно назначенного обратного вызова.</p>

<p><code class="language-plaintext highlighter-rouge">name</code> и <code class="language-plaintext highlighter-rouge">start_urls</code> это определенные в АПИ дефолтные атрибуты класса. <code class="language-plaintext highlighter-rouge">start_urls</code> это аналог вот этого:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://quotes.toscrape.com/page/1/'</span><span class="p">,</span>
        <span class="s">'http://quotes.toscrape.com/page/2/'</span><span class="p">,</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">parse</span><span class="p">)</span>
</code></pre></div></div>

<p>В АПИ scrapy использщуются генераторы. За счет конструкции <code class="language-plaintext highlighter-rouge">yield</code> обеспечивается большинство операций извлечения контента из запросов, а за счет колбеков - вызов, в том числе рекурсивный, необходимых операций.</p>

<p>В библиотеке реализован <a href="https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell">собственный шел</a>, с помощью которого можно попробовать запросы или извлечение данных из объектов запроса, а так-же можно выполнить дебагинг. Пример ниже:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scrapy shell <span class="s1">'http://quotes.toscrape.com/page/1/'</span>

2022-01-16 01:35:35 <span class="o">[</span>scrapy.utils.log] INFO: Scrapy 2.5.1 started <span class="o">(</span>bot: scrapybot<span class="o">)</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.utils.log] INFO: Versions: lxml 4.7.1.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.10 <span class="o">(</span>default, Jun  2 2021, 10:49:15<span class="o">)</span> - <span class="o">[</span>GCC 10.3.0], pyOpenSSL 21.0.0 <span class="o">(</span>OpenSSL 1.1.1m  14 Dec 2021<span class="o">)</span>, cryptography 36.0.1, Platform Linux-5.8.0-7630-generic-x86_64-with-glibc2.32
2022-01-16 01:35:35 <span class="o">[</span>scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor
2022-01-16 01:35:35 <span class="o">[</span>scrapy.crawler] INFO: Overridden settings:
<span class="o">{</span><span class="s1">'DUPEFILTER_CLASS'</span>: <span class="s1">'scrapy.dupefilters.BaseDupeFilter'</span>,
 <span class="s1">'LOGSTATS_INTERVAL'</span>: 0<span class="o">}</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.extensions.telnet] INFO: Telnet Password: a6f662f302955377
2022-01-16 01:35:35 <span class="o">[</span>scrapy.middleware] INFO: Enabled extensions:
<span class="o">[</span><span class="s1">'scrapy.extensions.corestats.CoreStats'</span>,
 <span class="s1">'scrapy.extensions.telnet.TelnetConsole'</span>,
 <span class="s1">'scrapy.extensions.memusage.MemoryUsage'</span><span class="o">]</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.middleware] INFO: Enabled downloader middlewares:
<span class="o">[</span><span class="s1">'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.cookies.CookiesMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware'</span>,
 <span class="s1">'scrapy.downloadermiddlewares.stats.DownloaderStats'</span><span class="o">]</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.middleware] INFO: Enabled spider middlewares:
<span class="o">[</span><span class="s1">'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware'</span>,
 <span class="s1">'scrapy.spidermiddlewares.offsite.OffsiteMiddleware'</span>,
 <span class="s1">'scrapy.spidermiddlewares.referer.RefererMiddleware'</span>,
 <span class="s1">'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware'</span>,
 <span class="s1">'scrapy.spidermiddlewares.depth.DepthMiddleware'</span><span class="o">]</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.middleware] INFO: Enabled item pipelines:
<span class="o">[]</span>
2022-01-16 01:35:35 <span class="o">[</span>scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2022-01-16 01:35:35 <span class="o">[</span>scrapy.core.engine] INFO: Spider opened
2022-01-16 01:35:36 <span class="o">[</span>scrapy.core.engine] DEBUG: Crawled <span class="o">(</span>200<span class="o">)</span> &lt;GET http://quotes.toscrape.com/page/1/&gt; <span class="o">(</span>referer: None<span class="o">)</span>
2022-01-16 01:35:36 <span class="o">[</span>asyncio] DEBUG: Using selector: EpollSelector
<span class="o">[</span>s] Available Scrapy objects:
<span class="o">[</span>s]   scrapy     scrapy module <span class="o">(</span>contains scrapy.Request, scrapy.Selector, etc<span class="o">)</span>
<span class="o">[</span>s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7ff2693c20a0&gt;
<span class="o">[</span>s]   item       <span class="o">{}</span>
<span class="o">[</span>s]   request    &lt;GET http://quotes.toscrape.com/page/1/&gt;
<span class="o">[</span>s]   response   &lt;200 http://quotes.toscrape.com/page/1/&gt;
<span class="o">[</span>s]   settings   &lt;scrapy.settings.Settings object at 0x7ff2693c0d90&gt;
<span class="o">[</span>s]   spider     &lt;DefaultSpider <span class="s1">'default'</span> at 0x7ff269178dc0&gt;
<span class="o">[</span>s] Useful shortcuts:
<span class="o">[</span>s]   fetch<span class="o">(</span>url[, <span class="nv">redirect</span><span class="o">=</span>True]<span class="o">)</span> Fetch URL and update <span class="nb">local </span>objects <span class="o">(</span>by default, redirects are followed<span class="o">)</span>
<span class="o">[</span>s]   fetch<span class="o">(</span>req<span class="o">)</span>                  Fetch a scrapy.Request and update <span class="nb">local </span>objects
<span class="o">[</span>s]   shelp<span class="o">()</span>           Shell <span class="nb">help</span> <span class="o">(</span>print this <span class="nb">help</span><span class="o">)</span>
<span class="o">[</span>s]   view<span class="o">(</span>response<span class="o">)</span>    View response <span class="k">in </span>a browser
2022-01-16 01:35:36 <span class="o">[</span>asyncio] DEBUG: Using selector: EpollSelector
In <span class="o">[</span>1]:
</code></pre></div></div>

<p>Для разбора html используются [<a href="css-selectors" title="Css-selectors">css-selectors</a>] и [<a href="xpath" title="XPath в scrapy">xpath</a>]. При этом CSS позволяет реализовать навигацию по html, а XPath более функциональный разбор ( а именно извлечение контента). Фактически CSS транслируется в XPath.</p>

<p>Пример извлечения данных с помощью селекторов:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">"quotes"</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://quotes.toscrape.com/page/1/'</span><span class="p">,</span>
        <span class="s">'http://quotes.toscrape.com/page/2/'</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'div.quote'</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s">'text'</span><span class="p">:</span> <span class="n">quote</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'span.text::text'</span><span class="p">).</span><span class="n">get</span><span class="p">(),</span>
                <span class="s">'author'</span><span class="p">:</span> <span class="n">quote</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'small.author::text'</span><span class="p">).</span><span class="n">get</span><span class="p">(),</span>
                <span class="s">'tags'</span><span class="p">:</span> <span class="n">quote</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'div.tags a.tag::text'</span><span class="p">).</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>
</code></pre></div></div>

<p>Результат:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2016-09-19 18:57:19 <span class="o">[</span>scrapy.core.scraper] DEBUG: Scraped from
&lt;200 http://quotes.toscrape.com/page/1/&gt;
<span class="o">{</span><span class="s1">'tags'</span>: <span class="o">[</span><span class="s1">'life'</span>, <span class="s1">'love'</span><span class="o">]</span>, <span class="s1">'author'</span>: <span class="s1">'André Gide'</span>,
<span class="s1">'text'</span>: <span class="s1">'“It is better to be hated for what you are than to be loved for what you are not.”'</span><span class="o">}</span>
2016-09-19 18:57:19 <span class="o">[</span>scrapy.core.scraper] DEBUG: Scraped from
&lt;200 http://quotes.toscrape.com/page/1/&gt;
<span class="o">{</span><span class="s1">'tags'</span>: <span class="o">[</span><span class="s1">'edison'</span>, <span class="s1">'failure'</span>, <span class="s1">'inspirational'</span>, <span class="s1">'paraphrased'</span><span class="o">]</span>, <span class="s1">'author'</span>: <span class="s1">'Thomas A. Edison'</span>,
<span class="s1">'text'</span>: <span class="s2">"“I have not failed. I've just found 10,000 ways that won't work.”"</span><span class="o">}</span>
</code></pre></div></div>

<p>Сохранение данных реализовано через <a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline">Item Pipline</a>. Можно сохранять в <code class="language-plaintext highlighter-rouge">.json</code> или в базы данных, сохранять файлы или делать скриншоты. Особых огранисений нет, в основном используется для:</p>

<ul>
  <li>очистка HTML-данных</li>
  <li>проверка очищенных данных (проверка того, что элементы содержат определенные поля)</li>
  <li>проверка на наличие дубликатов (и удаление их)</li>
  <li>сохранение очищенных элементов в базе данных или в виде файлов</li>
</ul>

<p>Пример сохранения в MongoDB:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pymongo</span>
<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>

<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">:</span>

    <span class="n">collection_name</span> <span class="o">=</span> <span class="s">'scrapy_items'</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MONGO_URI'</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'MONGO_DATABASE'</span><span class="p">,</span> <span class="s">'items'</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="p">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">client</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">collection_name</span><span class="p">].</span><span class="n">insert_one</span><span class="p">(</span><span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">).</span><span class="n">asdict</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">item</span>
</code></pre></div></div>

<p>Рекурсивный обход реализуется через колбеки. Пример:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Spider</span>


<span class="k">class</span> <span class="nc">AuthorSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'author'</span>

    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">'http://quotes.toscrape.com/'</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">author_page_links</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'.author + a'</span><span class="p">)</span>
        <span class="k">yield</span> <span class="k">from</span> <span class="n">response</span><span class="p">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">author_page_links</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">parse_author</span><span class="p">)</span>

        <span class="n">pagination_links</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="s">'li.next a'</span><span class="p">)</span>
        <span class="k">yield</span> <span class="k">from</span> <span class="n">response</span><span class="p">.</span><span class="n">follow_all</span><span class="p">(</span><span class="n">pagination_links</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_author</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">extract_with_css</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">css</span><span class="p">(</span><span class="n">query</span><span class="p">).</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s">''</span><span class="p">).</span><span class="n">strip</span><span class="p">()</span>

        <span class="k">yield</span> <span class="p">{</span>
            <span class="s">'name'</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s">'h3.author-title::text'</span><span class="p">),</span>
            <span class="s">'birthdate'</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s">'.author-born-date::text'</span><span class="p">),</span>
            <span class="s">'bio'</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s">'.author-description::text'</span><span class="p">),</span>
        <span class="p">}</span>
</code></pre></div></div>

<p>Этот паук запустится с главной страницы, он будет переходить по всем ссылкам на страницы авторов, вызывая <code class="language-plaintext highlighter-rouge">parse_author</code> для каждого из них, а также ссылки пагинации с <code class="language-plaintext highlighter-rouge">parse</code> рекурсивно. В данном примере реализован шаблон запроса по селектору <code class="language-plaintext highlighter-rouge">extract_with_css</code> а колбеки мы передаем в качестве аргументов в <code class="language-plaintext highlighter-rouge">response.follow_all()</code>.</p>

<p>Scrapy самостоятельно отфильтровывает повторяющиеся запросы к уже посещенным страницам (это можно настроить отдельно).</p>

<h2 id="архитектура">Архитектура</h2>

<p><img src="/myknowlegebase/attachments/2022-01-16-02-10-33.png" alt="Scrapy architecture" /></p>

<ol>
  <li>Engine получает первоначальные запросы на сканирование от Spider</li>
  <li>Engine планирует запросы в Scheduler и запрашивает следующие запросы для сканирования</li>
  <li>Scheduler возвращает следующие запросы движку</li>
  <li>Engine отправляет запросы Downloader, проходя через ПО промежуточного слоя Downloader</li>
  <li>Как только страница завершает загрузку, Downloader генерирует ответ (с этой страницей) и отправляет его в Engine, проходя через промежуточный смлой Downloader</li>
  <li>Engine получает ответ от Downloader и отправляет его Spider для обработки, проходя через промежуточное слой Spider</li>
  <li>Spider обрабатывает ответ и возвращает очищенные элементы и новые запросы в Engine, проходя через промежуточный слой Spider</li>
  <li>Engine отправляет обработанные элементы в Item Pipelines, затем отправляет обработанные запросы планировщику и запрашивает возможные следующие запросы для сканирования.</li>
  <li>Процесс повторяется (с шага 1) до тех пор, пока не закончатся запросы от Scheduler</li>
</ol>

<p>Engine отвечает за управление потоком данных между всеми компонентами системы и инициирование событий при выполнении определенных действий</p>

<p>Scheduler получает запросы от движка и ставит их в очередь для передачи позже (также в движок), когда движок их запрашивает</p>

<p>Downloader отвечает за получение веб-страниц и передачу их движку, который, в свою очередь, передает их в Spiders</p>

<p>Spiders — это настраиваемые классы, написанные пользователями Scrapy для анализа ответов и извлечения из них элементов или дополнительных запросов</p>

<p>Item Pipelines отвечает за обработку элементов после их извлечения (или очистки) пауками. Типичные задачи включают очистку, проверку и сохранение (например, сохранение элемента в базе данных)</p>

<p>Промежуточный слой Downloader — это специальные перехватчики, которые находятся между движком и загрузчиком и обрабатывают запросы, когда они передаются от движка к загрузчику, и ответы, которые передаются от загрузчика к движку. Их надо использовать для следующих задач:</p>

<ul>
  <li>обрабатывать запрос непосредственно перед его отправкой загрузчику (т. е. прямо перед тем, как Scrapy отправит запрос на веб-сайт)</li>
  <li>изменить полученный ответ перед передачей его пауку</li>
  <li>отправить новый запрос вместо передачи полученного ответа пауку</li>
  <li>передать ответ пауку, не загружая веб-страницу</li>
  <li>отбрасывать некоторые запросы</li>
</ul>

<p>Промежуточный слой Spider — это специальные хуки, которые находятся между движком и пауками и могут обрабатывать ввод (ответы) и вывод паука (элементы и запросы). Их надо использовать для следующих задач:</p>

<ul>
  <li>постобработка колбеков паука - изменение/добавление/удаление запросов или элементов</li>
  <li>постобработка start_requests</li>
  <li>обрабатывать исключения пауков</li>
  <li>вызывать errback вместо обратного вызова для некоторых запросов на основе содержимого ответа</li>
</ul>

<p>Network для скрапи написан на <a href="https://twistedmatrix.com/trac/">Twisted</a>, таким образом он является асинхронным (см. [<a href="asyncio" title="Asyncio">asyncio</a>])</p>

<p>Ссылки на оригинальный материал: <a href="https://docs.scrapy.org/en/latest/intro/tutorial.html">один</a>, <a href="https://docs.scrapy.org/en/latest/topics/architecture.html">два</a></p>

<p><a href="https://docs.scrapy.org/en/latest/index.html">Документация</a></p>

<h2 id="компоненты">Компоненты</h2>

<h3 id="command-line-tool"><a href="https://docs.scrapy.org/en/latest/topics/commands.html">Command line tool</a></h3>

<p>Конфиг устанавливается в <code class="language-plaintext highlighter-rouge">scrapy.cfg</code> в ini стиле</p>

<p><code class="language-plaintext highlighter-rouge">scrapy --info</code> - доступные опции</p>

<p><code class="language-plaintext highlighter-rouge">scrapy -h</code> все доступные команды</p>

<p><a href="https://docs.scrapy.org/en/latest/topics/commands.html#available-tool-commands">Глобальные</a>:</p>

<ul>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#startproject">startproject</a> создать новый проект</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#genspider">genspider</a> создать нового спайдера (опционально из темплейта <code class="language-plaintext highlighter-rouge">scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</code>)</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#settings">settings</a></li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#runspider">runspider</a> Запустите автономный паук в файле Python, не создавая проект.</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#shell">shell</a> Запускает оболочку Scrapy для заданного URL-адреса (если он указан) или пуст, если URL-адрес не указан</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#fetch">fetch</a> Загружает указанный URL-адрес с помощью загрузчика Scrapy и записывает содержимое в стандартный вывод.</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#view">view</a> Запускает оболочку Scrapy для заданного URL-адреса (если он указан) или пустой, если URL-адрес не указан.</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#version">version</a></li>
</ul>

<p>Локальные (только для проекта):</p>

<ul>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#crawl">crawl</a> запустить краулер</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#check">check</a> запустить проверку контрактов</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#list">list</a> список доступных спайдеров для проекта</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#edit">edit</a> малополезный доступ к редактору</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#parse">parse</a> Извлекает указанный URL-адрес и анализирует его с помощью паука, который его обрабатывает, используя метод, переданный с опцией <code class="language-plaintext highlighter-rouge">--callback</code>, или <code class="language-plaintext highlighter-rouge">parse</code>, если колбек не указан</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/commands.html#bench">bench</a> Быстрый бенчмарк-тест</li>
</ul>

<h3 id="spiders"><a href="https://docs.scrapy.org/en/latest/topics/spiders.html">Spiders</a></h3>

<p>Пауки — это классы, которые определяют, как будет исследоваться определенный сайт (или группа сайтов), в том числе как выполнять сканирование (т. е. переход по ссылкам) и как извлекать структурированные данные.</p>

<p>Для пауков цикл парсинга проходит примерно так: все начинается с создания начальных запросов для сканирования первых URL-адресов и указнияе функции обратного вызова, которая будет вызываться с ответом, полученным из этих запросов. Первые запросы для выполнения получаются путем вызова метода start_requests(), который (по умолчанию) генерирует запрос для URL-адресов, указанных в start_urls, и метода разбора в качестве функции обратного вызова.</p>

<p>В функции обратного вызова анализируется ответ (веб-страница) и возвращаются объекты. объекты запросмов или итераторы объектов. Объекты запросов также могут содержать обратные вызовы (возможно, рекурсивные).</p>

<p>При анализе содержимого страниц обычно используя селекторы (<strong>но вы также можете использовать [<a href="beautifulsoup" title="BeautifulSoup">BeautifulSoup</a>], lxml или любой другой механизм, который вы предпочитаете</strong>). Наконец, элементы, возвращаемые пауком, обычно сохраняются в базе данных (в каком-либо конвейере элементов) или записываются в файл с использованием Feed export. Несмотря на то, что этот цикл применим (более или менее) к любому типу пауков, существуют разные виды пауков по умолчанию, встроенные в Scrapy для разных целей</p>

<p>Основной класс <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy-spider">scrapy.Spider</a> Это самый простой паук, от которого наследуются все остальные пауки (включая пауков, поставляемых в комплекте с Scrapy). Никакого специального функционала он не предоставляет. Он просто предоставляет реализацию <code class="language-plaintext highlighter-rouge">start_requests()</code> по умолчанию, которая отправляет запросы из атрибута паука <code class="language-plaintext highlighter-rouge">start_urls</code> и вызывает анализ метода паука для каждого из полученных ответов.</p>

<p>Атрибуты, котоыре можно определить для паука:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">name</code> - адишник, по которому scrapy находит паука</li>
  <li><code class="language-plaintext highlighter-rouge">allowed_domains</code> - списко доменов, ограничивающих область поиска</li>
  <li><code class="language-plaintext highlighter-rouge">start_urls</code> - список стартовых урлов краулера</li>
  <li><code class="language-plaintext highlighter-rouge">custom_settings</code> - кастомные настройки краулера, переопределяющие настройки проекта. <a href="https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-ref">Есть набор готовых настроек</a></li>
  <li><code class="language-plaintext highlighter-rouge">crawler</code> реализует связку с <a href="https://docs.scrapy.org/en/latest/topics/api.html#topics-api-crawler">АПИ scrapi</a></li>
  <li><code class="language-plaintext highlighter-rouge">settings</code> настройки паука. Смотри <a href="https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings">раздел настроек</a></li>
  <li><code class="language-plaintext highlighter-rouge">logger</code> обычный python logger, который создается с атрибутом name. Смотри <a href="https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging-from-spiders">про логирпование</a> в scrapy и [<a href="logging" title="Logging - основные принципы">logging</a>]</li>
</ul>

<p>Методы:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">from_crawler(crawler, *args, **kwargs)</code> создает паука</li>
  <li><code class="language-plaintext highlighter-rouge">start_requests()</code> Этот метод должен возвращать итерируемый объект с первым Requests для этого паука. Он вызывается Scrapy, когда паук запускается. Scrapy вызывает его только один раз, поэтому безопасно также реализовать <code class="language-plaintext highlighter-rouge">start_requests()</code> в качестве генератора через <code class="language-plaintext highlighter-rouge">yield</code></li>
  <li><code class="language-plaintext highlighter-rouge">parse(response)</code> Это колбек по умолчанию, используемый Scrapy для обработки загруженных ответов, когда в их запросах не указан обратный вызов.</li>
  <li><code class="language-plaintext highlighter-rouge">log(message[, level, component])</code> Обертка, которая отправляет сообщение журнала через регистратор Spider, нужна для обратной совместимости.</li>
  <li><code class="language-plaintext highlighter-rouge">closed(reason)</code> вызывается для закрытия спайдера</li>
</ul>

<p>Пример:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="p">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">'example.com'</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">'example.com'</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">'http://www.example.com/1.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/2.html'</span><span class="p">,</span>
        <span class="s">'http://www.example.com/3.html'</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//h3'</span><span class="p">).</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s">"title"</span><span class="p">:</span> <span class="n">h3</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">xpath</span><span class="p">(</span><span class="s">'//a/@href'</span><span class="p">).</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="n">Request</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">parse</span><span class="p">)</span>
</code></pre></div></div>

<p>Спайдер может получать аргументы командной строки. Это делается через переопределение <code class="language-plaintext highlighter-rouge">__init__</code>. <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#spider-arguments">Смотри тут</a></p>

<p>Генерик спайдеры, реализованные в библиотеке:</p>

<ul>
  <li><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider">CrawlSpider</a> Это наиболее часто используемый паук для сканирования обычных веб-сайтов, поскольку он обеспечивает удобный механизм перехода по ссылкам путем определения набора правил. Класс реализует атрибут <code class="language-plaintext highlighter-rouge">rules</code>. Как определять правила краулинга, так-же пример, <a href="https://docs.scrapy.org/en/latest/topics/spiders.html#crawling-rules">смотри тут</a></li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#xmlfeedspider">XMLFeedSpider</a> Предназначен для разбора XML-каналов путем их итерации по определенному имени узла. Итератор можно выбрать из: <code class="language-plaintext highlighter-rouge">iternodes</code>, <code class="language-plaintext highlighter-rouge">xml</code> и <code class="language-plaintext highlighter-rouge">html</code>. Рекомендуется использовать итератор <code class="language-plaintext highlighter-rouge">iternodes</code> из соображений производительности, поскольку итераторы <code class="language-plaintext highlighter-rouge">xml</code> и <code class="language-plaintext highlighter-rouge">html</code> генерируют сразу весь DOM для его анализа.</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#csvfeedspider">CSVFeedSpider</a> Этот паук очень похож на XMLFeedSpider, за исключением того, что он перебирает строки, а не узлы</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/spiders.html#sitemapspider">SitemapSpider</a> Позволяет сканировать сайт, обнаруживая URL-адреса с помощью файлов Sitemap. Он поддерживает вложенные сайтмапы и обнаружение URL-адресов сайтмапов из robots.txt.</li>
</ul>

<h3 id="selectors"><a href="https://docs.scrapy.org/en/latest/topics/selectors.html">Selectors</a></h3>

<p>Собственный механизм извлечения структуры html-документа. Построен аокруг <a href="https://parsel.readthedocs.io/en/latest/">parsel</a> это библиотека Python под лицензией BSD для извлечения и удаления данных из HTML и XML с использованием селекторов [<a href="xpath" title="XPath в scrapy">xpath</a>] и [<a href="css-selectors" title="Css-selectors">css-selectors</a>], в сочетании с регулярными выражениями.</p>

<p>Руководство по использованию см. <a href="https://docs.scrapy.org/en/latest/topics/selectors.html">тут</a></p>

<h3 id="items"><a href="https://docs.scrapy.org/en/latest/topics/items.html">Items</a></h3>

<p>Основная цель скрейпинга — извлечь структурированные данные из неструктурированных источников, как правило, веб-страниц. Пауки могут возвращать извлеченные данные в виде элементов, объектов Python, которые определяют пары ключ-значение. Scrapy поддерживает несколько типов элементов, реализованных в <a href="https://github.com/scrapy/itemadapter">itemadapter</a>. Поддерживаются следующие типы:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">scrapy.item.Item</code></li>
  <li><code class="language-plaintext highlighter-rouge">dict</code></li>
  <li><code class="language-plaintext highlighter-rouge">dataclass</code>-based classes</li>
  <li><code class="language-plaintext highlighter-rouge">attrs</code>-based classes</li>
  <li><code class="language-plaintext highlighter-rouge">pydantic</code>-based classes</li>
</ul>

<p>Пример:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span><span class="p">,</span> <span class="n">is_item</span>
<span class="o">&gt;&gt;&gt;</span> <span class="o">@</span><span class="n">dataclass</span>
<span class="p">...</span> <span class="k">class</span> <span class="nc">InventoryItem</span><span class="p">:</span>
<span class="p">...</span>     <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">...</span>     <span class="n">price</span><span class="p">:</span> <span class="nb">float</span>
<span class="p">...</span>     <span class="n">stock</span><span class="p">:</span> <span class="nb">int</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">obj</span> <span class="o">=</span> <span class="n">InventoryItem</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'foo'</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mf">20.5</span><span class="p">,</span> <span class="n">stock</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_item</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">obj</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">adapter</span><span class="p">)</span>
<span class="mi">3</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">[</span><span class="s">"name"</span><span class="p">]</span>
<span class="s">'foo'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"price"</span><span class="p">)</span>
<span class="mf">20.5</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">[</span><span class="s">"name"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"bar"</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">"price"</span><span class="p">:</span> <span class="mf">12.7</span><span class="p">,</span> <span class="s">"stock"</span><span class="p">:</span> <span class="mi">9</span><span class="p">})</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">.</span><span class="n">item</span>
<span class="n">InventoryItem</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'bar'</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mf">12.7</span><span class="p">,</span> <span class="n">stock</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">.</span><span class="n">item</span> <span class="ow">is</span> <span class="n">obj</span>
<span class="bp">True</span>
<span class="o">&gt;&gt;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">adapter</span><span class="p">.</span><span class="n">asdict</span><span class="p">()</span>
<span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'bar'</span><span class="p">,</span> <span class="s">'price'</span><span class="p">:</span> <span class="mf">12.7</span><span class="p">,</span> <span class="s">'stock'</span><span class="p">:</span> <span class="mi">9</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span>
</code></pre></div></div>

<h3 id="item-loaders"><a href="https://docs.scrapy.org/en/latest/topics/loaders.html">Item Loaders</a></h3>

<p>Загрузчики айтемов предоставляют удобный механизм для заполнения айтемов извлеченными данными. Несмотря на то, что айтемы могут быть заполнены напрямую, загрузчики элементов предоставляют гораздо более удобный API для их заполнения из процесса очистки за счет автоматизации некоторых общих задач, таких как синтаксический анализ необработанных извлеченных данных перед их назначением. Другими словами, айтемы предоставляют контейнер очищенных данных, а загрузчики элементов предоставляют механизм для заполнения этого контейнера. Загрузчики элементов разработаны, чтобы обеспечить гибкий, эффективный и простой механизм для расширения и переопределения различных правил синтаксического анализа полей, либо с помощью паука, либо с помощью исходного формата (HTML, XML и т. д.).</p>

<p>Чтобы использовать загрузчик элементов, вы должны сначала создать его экземпляр. Вы можете создать его экземпляр с объектом айтема или без него, и в этом случае объект айтема автоматически создается в методе <code class="language-plaintext highlighter-rouge">__init__</code> загрузчика айтемов с использованием класса, указанного в атрибуте <code class="language-plaintext highlighter-rouge">ItemLoader.default_item_class</code>. Затем вы начинаете собирать значения в загрузчик, обычно используя селекторы. Вы можете добавить более одного значения в одно и то же поле; загрузчик айтемов знает, как «соединить» эти значения позже, используя соответствующую функцию обработки. Собранные данные хранятся внутри в виде списков, что позволяет добавлять несколько значений в одно и то же поле. Если аргумент айтема передается при создании загрузчика, каждое из значений айтема будет сохранено как есть, если оно уже является итерируемым, или заключено в список.</p>

<p>Пример</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="kn">import</span> <span class="n">Product</span>

<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'//div[@class="product_name"]'</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'//div[@class="product_title"]'</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'price'</span><span class="p">,</span> <span class="s">'//p[@id="price"]'</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">'stock'</span><span class="p">,</span> <span class="s">'p#stock]'</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'last_updated'</span><span class="p">,</span> <span class="s">'today'</span><span class="p">)</span> <span class="c1"># you can also use literal values
</span>    <span class="k">return</span> <span class="n">l</span><span class="p">.</span><span class="n">load_item</span><span class="p">()</span>
</code></pre></div></div>

<p>Пример использования:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span> <span class="c1"># (1)
</span><span class="n">l</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span> <span class="c1"># (2)
</span><span class="n">l</span><span class="p">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span> <span class="c1"># (3)
</span><span class="n">l</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'test'</span><span class="p">)</span> <span class="c1"># (4)
</span><span class="k">return</span> <span class="n">l</span><span class="p">.</span><span class="n">load_item</span><span class="p">()</span> <span class="c1"># (5)
</span></code></pre></div></div>

<ol>
  <li>Данные из xpath1 извлекаются и передаются через входной процессор поля имени. Результат процессора ввода собирается и сохраняется в лоадере (но еще не присваивается полю).</li>
  <li>Данные из xpath2 извлекаются и передаются через тот же входной процессор, что и в (1). Результат процессора ввода добавляется к данным, собранным в (1) (если они есть).</li>
  <li>Этот случай аналогичен предыдущим, за исключением того, что данные извлекаются из CSS-селектора css и передаются через тот же процессор ввода, что и в (1) и (2). Результат процессора ввода добавляется к данным, собранным в (1) и (2) (если есть).</li>
  <li>Этот случай также аналогичен предыдущим, за исключением того, что собираемое значение назначается напрямую, а не извлекается из выражения XPath или селектора CSS. Однако значение по-прежнему передается через процессоры ввода. В этом случае, поскольку значение не является итератором, оно преобразуется в итератор значение одного элемента перед передачей его процессору ввода, поскольку процессор ввода всегда получает итерируемые объекты.</li>
  <li>Данные, собранные на этапах (1), (2), (3) и (4), проходят через выходной процессор. Результатом процессора вывода является значение, присвоенное полю имени</li>
</ol>

<p>Таким образом мы можем собирать необходимые данные в лоадере через процессор ввода, а затем проводить необходимую обработку и заполнять поля айтема уже готовыми значениями через процессор вывода. Процессором может быть любая функция - единственное тербование к ней% она должна получать всего один аргумент, который должен быть итерируемым.</p>

<p>Лоадеры объявляются как классы. а процессоры как методы с <code class="language-plaintext highlighter-rouge">_in</code> и <code class="language-plaintext highlighter-rouge">_out</code> суффаиксами:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>
<span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="kn">import</span> <span class="n">ItemLoader</span>

<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>

    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">str</span><span class="p">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="nb">str</span><span class="p">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c1"># ...
</span></code></pre></div></div>

<h4 id="build-in-процессоры">Build-in процессоры</h4>

<p>Для удобства реализовано <a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors">несколько build-in процессоров</a></p>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.Compose">Compose</a> позволяет объединить несколько функций</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">Compose</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">.</span><span class="n">upper</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">'hello'</span><span class="p">,</span> <span class="s">'world'</span><span class="p">])</span>
<span class="s">'HELLO'</span>
</code></pre></div></div>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.Identity">Identity</a> просто возвращает данные. Видимо нужен для передачи данных из входа на выход как есть.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">Identity</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">'one'</span><span class="p">,</span> <span class="s">'two'</span><span class="p">,</span> <span class="s">'three'</span><span class="p">])</span>
<span class="p">[</span><span class="s">'one'</span><span class="p">,</span> <span class="s">'two'</span><span class="p">,</span> <span class="s">'three'</span><span class="p">]</span>
</code></pre></div></div>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.Join">Join(separator=’ ‘)</a> возвращает данные, объединенные сепаратором</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">Join</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">'one'</span><span class="p">,</span> <span class="s">'two'</span><span class="p">,</span> <span class="s">'three'</span><span class="p">])</span>
<span class="s">'one two three'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">(</span><span class="s">'&lt;br&gt;'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">'one'</span><span class="p">,</span> <span class="s">'two'</span><span class="p">,</span> <span class="s">'three'</span><span class="p">])</span>
<span class="s">'one&lt;br&gt;two&lt;br&gt;three'</span>
</code></pre></div></div>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.MapCompose">MapCompose(*functions, **default_loader_context)</a>. Процессор, построенный из композиции заданных функций, аналогичный процессору Compose. Отличие этого процессора заключается в том, как внутренние результаты передаются между функциями</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">filter_world</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="p">...</span>     <span class="k">return</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'world'</span> <span class="k">else</span> <span class="n">x</span>
<span class="p">...</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">filter_world</span><span class="p">,</span> <span class="nb">str</span><span class="p">.</span><span class="n">upper</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">'hello'</span><span class="p">,</span> <span class="s">'world'</span><span class="p">,</span> <span class="s">'this'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'something'</span><span class="p">])</span>
<span class="p">[</span><span class="s">'HELLO'</span><span class="p">,</span> <span class="s">'THIS'</span><span class="p">,</span> <span class="s">'IS'</span><span class="p">,</span> <span class="s">'SOMETHING'</span><span class="p">]</span>
</code></pre></div></div>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.SelectJmes">SelectJmes(json_path)</a>. Позоляет эффективно извлекать даныне из json на основе библиотеки <a href="https://github.com/jmespath/jmespath.py">jmespath.py</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">SelectJmes</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">MapCompose</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">SelectJmes</span><span class="p">(</span><span class="s">"foo"</span><span class="p">)</span> <span class="c1"># for direct use on lists and dictionaries
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">({</span><span class="s">'foo'</span><span class="p">:</span> <span class="s">'bar'</span><span class="p">})</span>
<span class="s">'bar'</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">({</span><span class="s">'foo'</span><span class="p">:</span> <span class="p">{</span><span class="s">'bar'</span><span class="p">:</span> <span class="s">'baz'</span><span class="p">}})</span>
<span class="p">{</span><span class="s">'bar'</span><span class="p">:</span> <span class="s">'baz'</span><span class="p">}</span>
</code></pre></div></div>

<p><a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#itemloaders.processors.TakeFirst">TakeFirst</a> возвращает первое ненулевое/непустое значение из полученных значений</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">TakeFirst</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">proc</span><span class="p">([</span><span class="s">''</span><span class="p">,</span> <span class="s">'one'</span><span class="p">,</span> <span class="s">'two'</span><span class="p">,</span> <span class="s">'three'</span><span class="p">])</span>
<span class="s">'one'</span>
</code></pre></div></div>

<p>Кроме того, процессоры могут быть объявлены в полях модели Item, если айтем реализован на оснвое <code class="language-plaintext highlighter-rouge">scrapy.Item</code>. Пример <a href="https://docs.scrapy.org/en/latest/topics/loaders.html#declaring-input-and-output-processors">смотри тут</a></p>

<h4 id="контексты">Контексты</h4>

<p><strong><a href="https://docs.scrapy.org/en/latest/topics/loaders.html#item-loader-context">Item Loader Context</a></strong> — это словарь, который используется всеми процессорами ввода и вывода в загрузчике элементов. Его можно передать при объявлении, создании экземпляра или использовании загрузчика элементов. Они используются для изменения поведения процессоров ввода/вывода.</p>

<h4 id="объект-itemloader"><a href="https://docs.scrapy.org/en/latest/topics/loaders.html#itemloader-objects">Объект ItemLoader</a></h4>

<p><code class="language-plaintext highlighter-rouge">classscrapy.loader.ItemLoader(item=None, selector=None, response=None, parent=None, **context)</code></p>

<ul>
  <li>item (<code class="language-plaintext highlighter-rouge">scrapy.item.Item</code>) — экземпляр элемента для заполнения с помощью последующих вызовов <code class="language-plaintext highlighter-rouge">add_xpath()</code>, <code class="language-plaintext highlighter-rouge">add_css()</code> или <code class="language-plaintext highlighter-rouge">add_value()</code></li>
  <li>selector (<code class="language-plaintext highlighter-rouge">Selector</code>) — селектор для извлечения данных при использовании метода <code class="language-plaintext highlighter-rouge">add_xpath()</code>, <code class="language-plaintext highlighter-rouge">add_css()</code>, <code class="language-plaintext highlighter-rouge">replace_xpath()</code> или <code class="language-plaintext highlighter-rouge">replace_css()</code></li>
  <li>response (<code class="language-plaintext highlighter-rouge">Response</code>) — ответ, используемый для создания другого селектора с использованием <code class="language-plaintext highlighter-rouge">default_selector_class</code>, если не указан аргумент селектора (в большинстве случаев этот аргумент игнорируется).</li>
</ul>

<p>Если item не задан, он создается автоматически с использованием класса в default_item_class. Все аргументы передаются в контекст лоадера.</p>

<p>Атрибуты и методы:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">item</code></li>
  <li><code class="language-plaintext highlighter-rouge">context</code></li>
  <li><code class="language-plaintext highlighter-rouge">default_item_class</code> используется, если не задан айтем</li>
  <li><code class="language-plaintext highlighter-rouge">default_input_processor</code> используется если в операции не задан входной процессор</li>
  <li><code class="language-plaintext highlighter-rouge">default_output_processor</code></li>
  <li><code class="language-plaintext highlighter-rouge">default_selector_class</code> дефолтный класс, используемый для конструирования селектора</li>
  <li><code class="language-plaintext highlighter-rouge">selector</code> - объект, из которого извлекаются данные</li>
  <li><code class="language-plaintext highlighter-rouge">add_css(field_name, css, *processors, **kw)</code></li>
  <li><code class="language-plaintext highlighter-rouge">add_value(field_name, value, *processors, **kw)</code></li>
  <li><code class="language-plaintext highlighter-rouge">add_xpath(field_name, xpath, *processors, **kw)</code></li>
  <li><code class="language-plaintext highlighter-rouge">get_collected_values(field_name)</code> возвращет собранные значения для поля</li>
  <li><code class="language-plaintext highlighter-rouge">get_css(css, *processors, **kw)</code> обработать css процессорами</li>
  <li><code class="language-plaintext highlighter-rouge">get_output_value(field_name)</code> Возвращает собранные значения, проанализированные с помощью выходного процессора, для данного поля. Этот метод не заполняет и не изменяет элемент.</li>
  <li><code class="language-plaintext highlighter-rouge">get_value(value, *processors, **kw)</code> обработать значение процессорами</li>
  <li><code class="language-plaintext highlighter-rouge">get_xpath(xpath, *processors, **kw)</code> обработать xpath процессорами</li>
  <li><code class="language-plaintext highlighter-rouge">load_item()</code> заполнить все поля айтема имеющимися данными</li>
  <li><code class="language-plaintext highlighter-rouge">nested_css(css, **context)</code> создание вложенных загрузчиков</li>
  <li><code class="language-plaintext highlighter-rouge">nested_xpath(xpath, **context)</code></li>
  <li><code class="language-plaintext highlighter-rouge">replace_css(field_name, css, *processors, **kw)</code> аналогично add, но не добавляет, а заменяет</li>
  <li><code class="language-plaintext highlighter-rouge">replace_value(field_name, value, *processors, **kw)</code></li>
  <li><code class="language-plaintext highlighter-rouge">replace_xpath(field_name, xpath, *processors, **kw)</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># HTML snippet: &lt;p class="product-name"&gt;Color TV&lt;/p&gt;
</span><span class="n">loader</span><span class="p">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'p.product-name'</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id="price"&gt;the price is $1200&lt;/p&gt;
</span><span class="n">loader</span><span class="p">.</span><span class="n">add_css</span><span class="p">(</span><span class="s">'price'</span><span class="p">,</span> <span class="s">'p#price'</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">'the price is (.*)'</span><span class="p">)</span>
<span class="n">loader</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'Color TV'</span><span class="p">)</span>
<span class="n">loader</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'colours'</span><span class="p">,</span> <span class="p">[</span><span class="s">'white'</span><span class="p">,</span> <span class="s">'blue'</span><span class="p">])</span>
<span class="n">loader</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'length'</span><span class="p">,</span> <span class="s">'100'</span><span class="p">)</span>
<span class="n">loader</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'name: foo'</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s">'name: (.+)'</span><span class="p">)</span>
<span class="n">loader</span><span class="p">.</span><span class="n">add_value</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">{</span><span class="s">'name'</span><span class="p">:</span> <span class="s">'foo'</span><span class="p">,</span> <span class="s">'sex'</span><span class="p">:</span> <span class="s">'male'</span><span class="p">})</span>
<span class="c1"># HTML snippet: &lt;p class="product-name"&gt;Color TV&lt;/p&gt;
</span><span class="n">loader</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'//p[@class="product-name"]'</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id="price"&gt;the price is $1200&lt;/p&gt;
</span><span class="n">loader</span><span class="p">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s">'price'</span><span class="p">,</span> <span class="s">'//p[@id="price"]'</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s">'the price is (.*)'</span><span class="p">)</span>
</code></pre></div></div>

<p><a href="https://docs.scrapy.org/en/latest/topics/loaders.html#nested-loaders">Вложенные загрузчики</a> используются для умсеньшения дублирования в строках селекторов.</p>

<h4 id="повторное-использование-и-расширение"><a href="https://docs.scrapy.org/en/latest/topics/loaders.html#reusing-and-extending-item-loaders">Повторное использование и расширение</a></h4>

<p>Пример (через наследование и переопредлеление процессора)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itemloaders.processors</span> <span class="kn">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="kn">import</span> <span class="n">ProductLoader</span>

<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">strip</span><span class="p">(</span><span class="s">'-'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="p">.</span><span class="n">name_in</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="item-pipeline"><a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html">Item Pipeline</a></h3>

<p>Конвеер айтемов обрабатывает айтем через несколько компонентов, которые выполняются последовательно. Каждый компонент конвейера эайтемов представляет собой класс Python, реализующий простой метод обработки. Пример приводился выше. Смотри доку и больше примеров <a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html">тут</a></p>

<h3 id="feed-exports"><a href="https://docs.scrapy.org/en/latest/topics/feed-exports.html">Feed Exports</a></h3>

<p>Обеспечивает хранение очищенных данных. Вот форматы сериализации, которые поддерживает scrapy:</p>

<ul>
  <li>JSON</li>
  <li>JSON lines</li>
  <li>CSV</li>
  <li>XML</li>
  <li>Pickle</li>
  <li>Marshal</li>
</ul>

<p>Так-же реализован доступ к хранилищам в локальных файловых системах, с доступом по ftp S3, GSC и с передачей объектов через стандартный выход.</p>

<h3 id="requests-and-responses"><a href="https://docs.scrapy.org/en/latest/topics/request-response.html">Requests and Responses</a></h3>

<p>Как правило, объекты Куйгуыеы генерируются в пауках и проходят через систему, пока не достигнут загрузчика, который выполняет запрос и возвращает объект ответа, который возвращается к пауку, выдавшему запрос. Оба класса Request и Response имеют подклассы, которые добавляют функциональность, не требуемую в базовых классах. Они описаны ниже в подклассах запроса и подклассах ответа.</p>

<p>В частности мы можем определять тело запроса, [<a href="http-заголовки" title="Http заголовки">http-заголовки</a>], cookies, кодировки и другие параметры запроса, а так-же передавать дополнительтные данные в функцию колбека.</p>

<p>Пример:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="p">.</span><span class="n">Request</span><span class="p">(</span><span class="s">"http://www.example.com/some_page.html"</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># this would log http://www.example.com/some_page.html
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">logger</span><span class="p">.</span><span class="n">info</span><span class="p">(</span><span class="s">"Visited %s"</span><span class="p">,</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">)</span>
</code></pre></div></div>

<p>Кроме того, реализованы:</p>

<ul>
  <li><a href="https://docs.scrapy.org/en/latest/topics/request-response.html#using-errbacks-to-catch-exceptions-in-request-processing">обработчики ошибок</a>, котоыре возникают в процессе запроса/ответа</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/request-response.html#request-meta-special-keys">параметры запроса</a>, например таймаут запрсоа или прокси</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/request-response.html#stopping-the-download-of-a-response">операции остановки загрузки по запросу</a></li>
</ul>

<p>Реализовано несколько сабклассов запроса:</p>

<ul>
  <li>scrapy.http.FormRequest</li>
  <li>scrapy.http.JsonRequest</li>
</ul>

<p>Объект Response представляет собой HTTP-ответ, который обычно загружается загрузчиком и передается паукам для обработки. Реализует весь набор [<a href="../lists/http" title="Http">http</a>]. Кроме того, реализованы сабклассы:</p>

<ul>
  <li>TextResponse для доп.кодирования и получения не только бинарных данных</li>
  <li>HtmlResponse</li>
  <li>XmlResponse</li>
</ul>

<h3 id="link-extractors"><a href="https://docs.scrapy.org/en/latest/topics/link-extractors.html">Link Extractors</a></h3>

<p>Реализует извлечение ссылок из ответов</p>

<h3 id="settings"><a href="https://docs.scrapy.org/en/latest/topics/settings.html">Settings</a></h3>

<p><a href="https://docs.scrapy.org/en/latest/topics/api.html#module-scrapy.settings">Settings API</a></p>

<p>Приоритет настроек:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'default'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s">'command'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s">'project'</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s">'spider'</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s">'cmdline'</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p><a href="https://stackoverflow.com/questions/71045333/scrapy-crawlerprocess-does-not-override-settings">Тут объясняется</a> почему нельзя переписать настройки, которые прписаны в самом пауке после создания инстанса паука</p>

<h3 id="exceptions"><a href="https://docs.scrapy.org/en/latest/topics/exceptions.html">Exceptions</a></h3>

<h2 id="в-дополнение-библиотека-поддердживает">В дополнение библиотека поддердживает</h2>

<ul>
  <li>сбор статистики</li>
  <li>отправку сообщений</li>
  <li>извлечение данных из инструментов разработчика браузера</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/media-pipeline.html">загрузку и фильтрацию файлов и изображений</a> (в т.ч. отброс повторов. эффективное храниение и т.д.)</li>
</ul>

<p>пример</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">itemadapter</span> <span class="kn">import</span> <span class="n">ItemAdapter</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">DropItem</span>
<span class="kn">from</span> <span class="nn">scrapy.pipelines.images</span> <span class="kn">import</span> <span class="n">ImagesPipeline</span>

<span class="k">class</span> <span class="nc">MyImagesPipeline</span><span class="p">(</span><span class="n">ImagesPipeline</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">get_media_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">image_url</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="s">'image_urls'</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="p">.</span><span class="n">Request</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">item_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="n">image_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s">'path'</span><span class="p">]</span> <span class="k">for</span> <span class="n">ok</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">if</span> <span class="n">ok</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">image_paths</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s">"Item contains no images"</span><span class="p">)</span>
        <span class="n">adapter</span> <span class="o">=</span> <span class="n">ItemAdapter</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">adapter</span><span class="p">[</span><span class="s">'image_paths'</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="k">return</span> <span class="n">item</span>
</code></pre></div></div>

<ul>
  <li>АПИ для деплоя на удаленный сервак</li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/autothrottle.html">автоматическое управление временем задержки загрузки</a></li>
  <li><a href="https://docs.scrapy.org/en/latest/topics/jobs.html">свой собственный шедалер</a></li>
  <li>поддержку сопрограмм и [<a href="asyncio" title="Asyncio">asyncio</a>]</li>
  <li>имеет открытый <a href="https://docs.scrapy.org/en/latest/topics/api.html#">низкоуровневый АПИ</a></li>
</ul>

<h2 id="middlewire">Middlewire</h2>

<p>Реализует расширяющие функциональность scrapy хуки двух типаов - <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html">даунлоадеры</a> и <a href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html">хуки паков</a>. К примеру обработка [<a href="parsing-robots-txt-with-scrapy" title="Parsing robots txt with scrapy">parsing-robots-txt-with-scrapy</a>] осуществляется специальным middlewire, котоырй запускается первым в очереди хуков загрузки.</p>

<p>Порядок исполнения middlewire задается в settings</p>

<ul>
  <li>For downloader middlewares, the <code class="language-plaintext highlighter-rouge">process_request</code> method is called in <strong>increasing</strong> order</li>
  <li>For downloader middlewares, the <code class="language-plaintext highlighter-rouge">process_response</code> method is called in <strong>decreasing</strong> order</li>
  <li>For spider middlewares, the <code class="language-plaintext highlighter-rouge">process_spider_input</code> method is called in <strong>increasing</strong> order</li>
  <li>For spider middlewares, the <code class="language-plaintext highlighter-rouge">process_spider_output</code> method is called in <strong>decreasing</strong> order</li>
</ul>

<p>Пример игнорирования запроса:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="kn">import</span> <span class="n">IgnoreRequest</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">log</span>

<span class="k">class</span> <span class="nc">CustomDownloaderMiddleware</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

   <span class="k">def</span> <span class="nf">process_response</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
       <span class="n">log</span><span class="p">.</span><span class="n">msg</span><span class="p">(</span><span class="s">"In Middleware "</span> <span class="o">+</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">log</span><span class="p">.</span><span class="n">WARNING</span><span class="p">)</span>
       <span class="k">if</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span> <span class="o">==</span> <span class="s">"http://www.achurchnearyou.com//"</span><span class="p">:</span>
           <span class="k">raise</span> <span class="n">IgnoreRequest</span><span class="p">()</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="k">return</span> <span class="n">response</span>
</code></pre></div></div>

<p><a href="https://stackoverflow.com/a/14077313/15966204">Ссылка на оверфло</a></p>

<h2 id="запуск-пауков-из-скрипта">Запуск пауков из скрипта</h2>

<p><a href="https://docs.scrapy.org/en/latest/topics/practices.html">Смотри тут</a></p>

<p><a href="https://docs.scrapy.org/en/latest/topics/api.html#crawler-api">Crawler API</a>:</p>

<ul>
  <li><a href="https://docs.scrapy.org/en/latest/topics/practices.html#running-multiple-spiders-in-the-same-process">CrawlerProcess</a> несколько пауков в одном процессе</li>
  <li>CrawlerRunner запуск паука в готовом “реакторе” - требует дополнительного конфигурирования самого реактора. Все так-же в одном процессе</li>
</ul>

<p>Смотри еще:</p>

<ul>
  <li>[<a href="../lists/crawlers" title="Crawlers">crawlers</a>]</li>
  <li>[<a href="selenium" title="Selenium">selenium</a>]</li>
  <li>[<a href="splash" title="Splash">splash</a>]</li>
  <li>[<a href="playwright" title="Playwright">playwright</a>]</li>
  <li>[<a href="xpath" title="XPath в scrapy">xpath</a>]</li>
  <li>[<a href="scrapyd" title="Scrapyd">scrapyd</a>] an application for deploying and running Scrapy spiders. It enables you to deploy (upload) your projects and control their spiders using a JSON API</li>
  <li><a href="https://github.com/DormyMo/SpiderKeeper">spiderkeeper</a> admin ui for scrapy/open source scrapinghub</li>
  <li><a href="https://github.com/crawlab-team/crawlab">crawlab</a> Distributed web crawler admin platform for spiders management regardless of languages and frameworks.</li>
  <li><a href="https://github.com/scrapinghub/scrapy-poet">scrapy-poet</a> Page Object pattern for Scrapy. Allows to write spiders where extraction logic is separated from the crawling one. With scrapy-poet is possible to make a single spider that supports many sites with different layouts</li>
  <li>[<a href="../posts/2022-02-12-daily-note" title="Несколько вопросов о реализации пауков в scrapy">2022-02-12-daily-note</a>] парсинг sitemap, фильтрацияи и способы обхода в scrapy</li>
  <li>[<a href="parsing-robots-txt-with-scrapy" title="Parsing robots txt with scrapy">parsing-robots-txt-with-scrapy</a>]</li>
  <li>[<a href="scrapy-rotating-proxy" title="Scrapy rotating proxy">scrapy-rotating-proxy</a>]</li>
  <li><a href="https://thepythonscrapyplaybook.com/">scrapy playbook</a> Everything you need to know to become a Scrapy Pro!</li>
</ul>


  </div>
</article>


    </main>

    <footer role="banner">
<div class="border-top-thin clearfix mt-2 mt-lg-4">
    <div class="container mx-auto px-2">
      <p class="col-8 sm-width-full left py-2 mb-0"><a href="/myknowlegebase/">My knowledge base</a> проект поддерживается <a class="text-accent" href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></p>
      <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
        <li class="inline-block mr-1">
          <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="My knowledge base">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
        </li>
      </ul>
    </div>
  </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>