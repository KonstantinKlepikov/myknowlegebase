<!DOCTYPE html>
<html lang="ru_RU">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Multy armed bandits | My knowledge base</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Multy armed bandits" />
<meta property="og:locale" content="ru_RU" />
<meta name="description" content="Многорукие бандиты" />
<meta property="og:description" content="Многорукие бандиты" />
<link rel="canonical" href="http://localhost:4000/myknowlegebase/notes/multi-armed-bandit.html" />
<meta property="og:url" content="http://localhost:4000/myknowlegebase/notes/multi-armed-bandit.html" />
<meta property="og:site_name" content="My knowledge base" />
<script type="application/ld+json">
{"headline":"Multy armed bandits","description":"Многорукие бандиты","@type":"WebPage","url":"http://localhost:4000/myknowlegebase/notes/multi-armed-bandit.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="keywords" content="">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

    
    
    <link rel="stylesheet" href="http://localhost:4000/myknowlegebase/assets/style.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>

    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->



<!-- Favicon -->
<link rel="icon" href="http://localhost:4000/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">
<link rel="shortcut icon" href="http://localhost:4000/myknowlegebase/assets/img/favicon.ico" type="image/x-icon">

<!-- Math support -->
<!-- Mathjax Support -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- tags collection-->

    







<!-- end custom head snippets -->

</head>

  <body>

    <header class="border-bottom-thick px-2 clearfix">
    <div class="left sm-width-full py-1 mt-1 mt-lg-0">
      <a class="align-middle link-primary text-accent" href="https://konstantinklepikov.github.io/">
        My deep learning
      </a>
    </div>
    <div class="right sm-width-full">
      <ul class="list-reset mt-lg-1 mb-2 mb-lg-1">
        <li class="inline-block">
          <a class="align-middle link-primary mr-2 mr-lg-0 ml-lg-2" href="/myknowlegebase/">
            My knowledge base
          </a>
        </li>
      </ul>
    </div>
  </header>

    <main role="main">

      
<article class="container mx-auto px-2 mt2 mb4">
  <header>
    <h1 class="h1 col-9 sm-width-full py-4 mt-3 inline-block" itemprop="name headline">Multy armed bandits</h1>
  </header>
  <div class="col-4 sm-width-full border-top-thin">
    <p class="mb-3 h5">Теги:
      
        
        <a href="/myknowlegebase/tag/machine-learning" title="machine-learning" class="link-tags">machine-learning&nbsp;</a>
      
        
        <a href="/myknowlegebase/tag/r-learning" title="r-learning" class="link-tags">r-learning&nbsp;</a>
      
      </p>
  </div>
  <div class="prose mb-4 py-4">
    <p>k-рукие бандиты решают следующую задачу: есть k вариантов действий, мы многократно стоим перед проблемой выбора из этих вариантов. После каждого выбора мы получаем численное вознаграждение, выбираемое из стационарного распределения вероятностей, которое зависит от выбранного действия. Цель - максимизировать ожидаемое полное вознаграждение за определенный период. Благодаря повторному выбору, мы стремимся максимизировать выбор только лучших решений. Ценность произвольного действия a:</p>

<script type="math/tex; mode=display">q_*(a) \doteq \mathbb{E}[R_t|A_t = a]</script>

<p>где <script type="math/tex">A_t</script> действие, выбранное на шаге t, а <script type="math/tex">R_t</script> соответствующее ему вознаграждение. На практике ценность каждого действия мы не знаем, поэтому приходится применять методы оценивания ценности действий. Если мы совершаем действия и оцениваем их, тогда на любом временном отрезке мы будем иметь максимально ценное действие или “жадное”. Когда мы выбираем жадное действие, то используем текущие знания о ценности действий, а когда выбираем какое-то другое - исследуем нежадные действия.</p>

<p>Истинная ценность действия - это среднее вознаграждение при условии выбора этого действия. Простейшее правило - выбирать одно из жадных действий:</p>

<script type="math/tex; mode=display">A_t \doteq argmax_a Q_t(a)</script>

<p>где <script type="math/tex">Q_t(a)</script> истинная ценность.</p>

<p>Альтернативный вариант - большую часть времени выбирать жадное действие, но иногда - исследовать. <strong>Методы, которые используют данный подход, называются <script type="math/tex">\varepsilon</script>-жадными</strong>. В пределе такие методы приводят к тому, что каждое действие (не только жадные), будут выбраны бесконечное число раз, что сведет истинную ценность к ценности <script type="math/tex">q_*(a)</script>.</p>

<p>Эпислон-жадные алгоритмы зависят от выбора параметра, т.е. от предпочтения разведывать или эксплуатировать. Можно опираться на текущую оценку распределения вознаграждений (softmax), чтобы исследовать не произвольное состояние, а состоянии в направлении максимизации вознаграждения. Еще используют стратегию “отжига” <script type="math/tex">\varepsilon</script> - в начале <script type="math/tex">\varepsilon</script> большой, что провоцирует больше исследований, но к концу обучения параметр постепенно снижается, что уменьшает разведывающую способность алгоритма. Другой вариант - поощрять исследования состояний, которые ранее не посещались (это методы с верхним доверительным интервалом, <strong>upper-confidence bound, UCB</strong>). UCB полезны, т.к. у них нет гиперпараметров. Ниже приведено сравнение для <a href="https://rl-book.com/learn/bandits/exploration_comparison/">простейшей задачи обучения с подкреплением</a>:</p>

<p><img src="/myknowlegebase/attachments/2023-01-01-20-53-52.png" alt="e-greedy example" /></p>

<p>Смотри еще:</p>

<ul>
  <li>[<a href="../lists/reinforcement-learning" title="Reinforcement learning">reinforcement-learning</a>]</li>
  <li>[<a href="mppr" title="MPPR">mppr</a>]</li>
  <li><a href="https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-2-multi-arm-bandits-be5efb2e83ea">Multi-arm Bandits</a></li>
</ul>


  </div>
</article>


    </main>

    <footer role="banner">
<div class="border-top-thin clearfix mt-2 mt-lg-4">
    <div class="container mx-auto px-2">
      <p class="col-8 sm-width-full left py-2 mb-0"><a href="/myknowlegebase/">My knowledge base</a> проект поддерживается <a class="text-accent" href="https://github.com/KonstantinKlepikov">KonstantinKlepikov</a></p>
      <ul class="list-reset right clearfix sm-width-full py-2 mb-2 mb-lg-0">
        <li class="inline-block mr-1">
          <a href="https://twitter.com/share" class="twitter-share-button" data-hashtags="My knowledge base">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
        </li>
      </ul>
    </div>
  </div>
</footer>

  </body>

</html>

<script type="text/javascript">
  // Hack: Replace page-link with "Page Title"
  document.querySelectorAll(".markdown-body a[title]").forEach((a) => {
    a.innerText = a.title;
  });
  // Hack: Remove .md extension from wikilinks to get the html in jekyll
  document.querySelectorAll("a").forEach(l => {
    if (l.href.endsWith('.md')) {
      l.href = l.href.substring(0, l.href.length-3)
    }
  })
</script>