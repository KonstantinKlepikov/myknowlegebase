---
description: Методы градиента стратегии в обучении с подкреплением
tags: machine-learning r-learning
title: Policy Gradient Methods
---
Основное отличие методов градиента стратегии (или градиента политики) от обычных методов RL заключается в том, что данные методы позволяют напрямую найти оптимильную стратегию минуя многократную максимизацию следующего ожидаемого значения. Это достигается за счет использования стохастической стратегии, что позволяет агенту выбирать действия в соответствии с произвольными вероятностями, устраняя необходимость в стратегиях принудительного исследования (к примеру $$\varepsilon$$-жадного поиска), при этом предпочтительные действия в такой стратегии плавно меняются со временем. Кроме того, стохастическую политику легче аппроксимировать.

Общая задача модели градиента стратегии - это вычисление градиента общего вознаграждения относительно параметров стратегии. На основании результата параметры изменяются с целью увеличения вознаграждения.

В качестве функций градиента стратегии обычно выбирают линейную стратегию (логистическую, softmax), однако легко можно выбрать и произвольную функию.

Основные реализации:

- REINFORCE (реализация метода [[monte-carlo]] для градиента политик). [Пример в среде CartPole](https://rl-book.com/learn/policy_gradients/reinforce/). REINFORCE собирает траекторию, основанную на текущей стратегии для всего эпизода, затем обновляет стратегию.
- REINFORCE с базовым ожидаемым доходом - позволяет улучшить убучение за счет снижения нестабильности градиентов, возникающей из-за больших измененеий вознаграждения. [Пример](https://rl-book.com/learn/policy_gradients/reinforce_baseline/)
- n-шаговый алгоритм градиента стратегии (актор-критик), который устраняет проблему необходимости ждать окончания эпизода. В этом контексте стратегия изучает и описывает наилучшие действия, которые следует предпринять (актор). Прогноз из n-шагов критикует последствия действийи используется для обновления стратегии (критик). Такой подход получил название A2C (advantage actor-critic). [Пример](https://rl-book.com/learn/policy_gradients/actor_critic/)
- мягкий актор-критик SAC (soft actor-critic) - градиент стратегии апроксимируется функцией энтропии Шеннона (функция максимизирует доход, регулируемый энтропией стратегии).

Методы градиента стратегии используют два гиперпараметра "затухания" - один для стратегии, второй для оценки функции ценности состояния. В алгоритмах актор-критик с трассировкой соответствия есть третий гиперпараметр, который позволяет контроллировать уровень бустрапирования (смотри SARSA($$\lambda$$) в [[temporal-difference]])

Смотри еще:

- [[reinforcement-learning]]
- [[deep-q-learning]]
- [[temporal-difference]]
- [[another-and-nonstandart-methods-of-reinforcement-learning]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[monte-carlo]: monte-carlo "Monte-Carlo methods"
[temporal-difference]: temporal-difference "Temporal difference methods and n-steps methods"
[reinforcement-learning]: ../lists/reinforcement-learning "Reinforcement learning"
[deep-q-learning]: deep-q-learning "Deep Q-learning"
[temporal-difference]: temporal-difference "Temporal difference methods and n-steps methods"
[another-and-nonstandart-methods-of-reinforcement-learning]: another-and-nonstandart-methods-of-reinforcement-learning "Another and nonstandart methods of reinforcemebt learning"
[//end]: # "Autogenerated link references"