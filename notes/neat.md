---
description: Алгоритм NEAT
tags: machine-learning genetic pip python evol
title: NEAT - нейроэволюционный алгоритм
---
Оригинальный алгоритм описан [тут](http://www.cs.ucf.edu/~kstanley/neat.html)

NEAT предназначен для уменьшения размерности пространства поиска параметров посредством развития структуры нейросети в проуессе эволюции. Базовый алгоритм использует прямое кодирование генома, поэтому вычислительно затратен и не годится для построения больших сетей.

Основное преимущество метода - построеная сеть не нуждается в обучении обратным распространением ошибки, что уменьшает вычислительные затраты. Кроме того, эволюционно развиваемая топология сети позволяет построить структуру сч высокой емкостю информации. Нет необходимости в построении однородных слоев (как это делается в классических нейронных сетях), состоящих из нейронов с функциями активации одного типа. Значительно сокращается число параметров сети, что ведет к ее крайне высокой эффективности.

Недостаток - вычислительная сложность поиска оптимальной топологии.

[Оригинальная статья, посвященная алгоритму](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf)

Следующий шаг в развитии NEAT - топология на основе гиперкуба. Алгоритм HyperNEAT строится на базе схемы косвенного кодирования генома (CPPN). HyperNEAT сохраняет паттерн связи сети-фенотипа в виде четырехмерного гиперкуба, где каждая точка кодирует связь между двумя узлами сети. Это позволяет компактно хранить множество паттернов, а кроме того использовать их повторно. Такой метод может использовать разные функции активации для своих скрытых узлов и выражать различные геометрические закономерности наблюдаемых данных. Сам гиперкуб (который принято назывть субстратом) может иметь предварительно выбранную геометрию, что улучшает скорость и эффективность обучения.

С HyperNEAT стало возможным строить очень большие сети. К недостатку метода можно отнести необходимость выбора начальной структуры субстрата инженером, что в целом задача нетривиальная и усложняет процесс успешного обучения.

[Статья, посвященная HyperNEAT](https://axon.cs.byu.edu/Dan/778/papers/NeuroEvolution/stanley3**.pdf)

Алгоритм es-HyperNEAT ведет развивает идею дальше. Используется развиваемый субстрат - вместо задания начального положения узлов в субстрате инженером. алгоритм оценивает плотность распределения информации в субстрате в процессе обучения. В участках с плотностью, превышщающей пороговую и размещаются узлы. Такой подход открывает дорогу к самообучаемым сетям, способным находить оптимальную топологию в процессе эволюции без точной инициализвации субстрата.

[Статья про es-HyperNEAT](http://eplex.cs.ucf.edu/papers/risi_alife12.pdf)

## Библиотеки

[NEAT-python](https://neat-python.readthedocs.io/en/latest/)

[PyTorch-NEAT](https://github.com/uber-research/PyTorch-NEAT)

[TensorFlow-NEAT](https://github.com/crisbodnar/TensorFlow-NEAT)

[MultiNEAT](https://github.com/KonstantinKlepikov/MultiNEAT) - к сожалению документация проекта на сайте не доступна

[deep-neuroevolution](https://github.com/uber-research/deep-neuroevolution)

[Pure Python Library for ES-HyperNEAT. Contains implementations of HyperNEAT and ES-HyperNEAT](https://github.com/ukuleleplayer/pureples)

Коллекция, состоящая из оригинального алгоритма NEAT и большинства его имплементаций (включая multineat и все остальное семейство), известных к настоящему моменту: [Find the Right Version of NEAT for Your Needs](https://eplex.cs.ucf.edu/neat_software/)

Еще про эволюционные методы:

- [[deap]]
- [[deap-docs]]
- [[evolution-methods]]

[//begin]: # "Autogenerated link references for markdown compatibility"
[deap]: deap "Deap - генетические алгоритмы на python"
[deap-docs]: deap-docs "Deap документация"
[evolution-methods]: ..%2Flists%2Fevolution-methods "Evolution methods"
[//end]: # "Autogenerated link references"